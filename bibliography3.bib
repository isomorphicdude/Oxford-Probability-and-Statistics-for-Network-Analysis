@misc{anastasiou2022steins,
      title={Stein's Method Meets Computational Statistics: A Review of Some Recent Developments}, 
      author={Andreas Anastasiou and Alessandro Barp and François-Xavier Briol and Bruno Ebner and Robert E. Gaunt and Fatemeh Ghaderinezhad and Jackson Gorham and Arthur Gretton and Christophe Ley and Qiang Liu and Lester Mackey and Chris. J. Oates and Gesine Reinert and Yvik Swan},
      year={2022},
      eprint={2105.03481},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@inproceedings{stein1972bound,
  title={A bound for the error in the normal approximation to the distribution of a sum of dependent random variables},
  author={Stein, Charles},
  booktitle={Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory},
  volume={6},
  pages={583--603},
  year={1972},
  organization={University of California Press}
}

@article{stein1986,
 ISSN = {07492170},
 URL = {http://www.jstor.org/stable/4355512},
 author = {Charles Stein},
 journal = {Lecture Notes-Monograph Series},
 pages = {i--164},
 publisher = {Institute of Mathematical Statistics},
 title = {Approximate Computation of Expectations},
 urldate = {2024-01-04},
 volume = {7},
 year = {1986}
}

@book{durrett2010probability,
author = {Durrett, Rick},
title = {Probability: Theory and Examples},
year = {2010},
isbn = {0521765390},
publisher = {Cambridge University Press},
address = {USA},
edition = {4th},
abstract = {This book is an introduction to probability theory covering laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. It is a comprehensive treatment concentrating on the results that are the most useful for applications. Its philosophy is that the best way to learn probability is to see it in action, so there are 200 examples and 450 problems.}
}


@InProceedings{pmlr-v119-grathwohl20a,
  title = 	 {Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling},
  author =       {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Joern-Henrik and Duvenaud, David and Zemel, Richard},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3732--3747},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/grathwohl20a/grathwohl20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/grathwohl20a.html},
  abstract = 	 {We present a new method for evaluating and training unnormalized density models. Our approach only requires access to the gradient of the unnormalized model’s log-density. We estimate the Stein discrepancy between the data density p(x) and the model density q(x) based on a vector function of the data. We parameterize this function with a neural network and fit its parameters to maximize this discrepancy. This yields a novel goodness-of-fit test which outperforms existing methods on high dimensional data. Furthermore, optimizing q(x) to minimize this discrepancy produces a novel method for training unnormalized models. This training method can fit large unnormalized models faster than existing approaches. The ability to both learn and compare models is a unique feature of the proposed method.}
}
