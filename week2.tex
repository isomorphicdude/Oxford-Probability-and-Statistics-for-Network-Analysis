\documentclass{article}

\input{includes/commands.tex}
\input{includes/theoremstyle.tex}
\usepackage{tikz}
\usepackage[colon, sort&compress]{natbib}
\title{Week 2 Random Graphs}

\date{\today}

\begin{document}
% \author{\aut}
\maketitle

\section{Random Graphs}
In order to judge whether a network summary is ”unusual” or whether a motif (pattern) is ”frequent”, there is an underlying assumption of randomness in the network. The randomness can be intrinsic to the network, and/or may stem from errors in the data. With the random models, we would like to compute important statistics such as \textbf{the degree distribution}, \textbf{the clustering coefficient}, and \textbf{the number of triangles}.

\subsection{Erd\H{o}s-R\'{e}nyi}
The first configuration of Erd\H{o}s-R\'{e}nyi random graphs specifies edges with a probability $p$. Namely, for $V=\{1,2,\ldots,n\}$, the probability of an edge between $i$ and $j$ is $p$ for all $i,j\in V$.
\begin{definition}
    A random graph $\mathcal{G}(n,p)$ is an undirected, unweighted, simple graph on $n$ vertices where the adjacency matrix satisfies
    \begin{equation*}
        \mathbb{P}(A_{ij}=1)=p \quad \forall i \neq j
    \end{equation*}
    The edges are formed independently of each other.
\end{definition}

In this case, the expected number of edges is $m=\binom{n}{2}p$ and the degree of a vertex $v$ follows a binomial distribution with parameters $n-1$ and $p$:
\[
\mathbb{P}(d(v)=k)=\binom{n-1}{k}p^k(1-p)^{n-1-k}
\]

Also, by independence, we can compute the number of triangles by:
\begin{align*}
    \mathbb{E}[T]&=\sum_{i<j<k} \mathbb{E}[A_{ij}A_{jk}A_{ki}]=\binom{n}{3}p^3
\end{align*}

\begin{unexaminable}
    The local clustering coefficient of a vertex $v$ is $p$. See \href{https://math.stackexchange.com/questions/2200452/expected-local-clustering-coefficient-for-a-node-in-a-network}{this answer} for a proof.
\end{unexaminable}

Now, recall the definition of the (global) clustering coefficient:
\begin{equation*}
    \mathbb{E}[C] = \frac{3 \times \mathbb{E}[\text{number of triangles}]}{\mathbb{E}[\text{number of connected triples}]}
\end{equation*}

Combining with the result above, the expected (global) clustering coefficient is
\begin{align*}
    \mathbb{E}[C]&=\frac{\mathbb{E}[\sum_{u,v,w \in V} a_{uv}a_{vw}a_{wu}]}{\mathbb{E}[\sum_{v\in V} d(v)(d(v)-1)]} \\
    &= \frac{6p^3 \binom{n}{3}}{n ((n-1)p(1-p) + (n-1)^2p^2 -(n-1)p)}\\
    &=p
\end{align*}


An alternative configuration is to specify the number of edges $m$ and choose uniformly from all graphs with $m$ edges. 

\begin{definition}
    A random graph $\mathcal{G}(n,m)$ is a graph on $n$ vertices where the adjacency matrix satisfies
    \begin{equation*}
        \mathbb{P}(A_{ij}=1)=\frac{m}{\binom{n}{2}} \quad \forall i \neq j
    \end{equation*}
    The edges are independent of each other.
\end{definition}

The degree distribution here is hypergeometric:
$$
\mathbb{P}(d(v)=k)=\frac{\binom{n-1}{k}\binom{\binom{n}{2}-k}{m-k}}{\binom{\binom{n}{2}}{m}}
$$

where $k=0,1,\ldots,\min(n-1,m)$. To see this, note that we can choose $k$ vertices to be connected to $v$ among $V\setminus \{v\}$, and then choose $m-k$ edges from the remaining $\binom{n}{2}-k$ edges to connect the rest pairs of vertices. 

The expected number of triangles is:
\begin{align*}
    \mathbb{E}[T]&=\sum_{i<j<k} \mathbb{E}[A_{ij}A_{jk}A_{ki}]\\
    &=\binom{n}{3}\frac{m(m-1)(m-2)}{\binom{n}{2}\left(\binom{n}{2}-1\right)\left(\binom{n}{2}-2\right)}\\
\end{align*}

this is due the dependence between the edges, so for a fixed triplet $(i,j,k)$, we first choose the edge $(i,j)$ with probability $\frac{m}{\binom{n}{2}}$, then $(j,k)$ with probability $\frac{m-1}{\binom{n}{2}-1}$, and finally $(k,i)$ with probability $\frac{m-2}{\binom{n}{2}-2}$.  

The expected (global) clustering coefficient can be computed similarly, which we omit here.  

\subsection{Small World}

In reality, it is often found that the average distance between two vertices is small (short path), but the clustering coefficient is large. This is known as the \textbf{small world phenomenon}, which cannot be modelled (properly) by the Erd\H{o}s-R\'{e}nyi random graphs.  

\subsubsection{Watts-Strogatz Model}
The \textbf{Watts-Strogatz model} is a random graph constructed with the following steps:
\begin{itemize}
    \item Start with a ring of $n$ vertices, where each vertex is connected to its $k$ nearest neighbours on each side (hence $2k$ edges per vertex)
    \item Choose a vertex and rewire each of its edges incident to its clock-wise neighbour with probability $p$.
    \item Repeat the previous process for all vertices (moving in the clock-wise direction).
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        % Draw the circle of nodes
        \foreach \x in {1,...,12}{
          \node[circle, draw, fill=black, inner sep=2pt] (node\x) at (\x*30:2cm) {};
        }
        
        % Set the style for the edges
        \tikzset{edge/.style={line width=2pt, color=astral}}
        
        \draw[edge] (node1) -- (node11);
        \draw[edge] (node1) -- (node12);
        \draw[edge] (node1) -- (node2);
        \draw[edge] (node1) -- (node3);
      \end{tikzpicture}
    \caption{We connect the vertex to its nearest neighbours on both directions. (\href{https://www.kth.se/social/files/5605669af2765468be471eda/lecture\%204\%20\%282015\%29.pdf}{a better illustration})}
    \label{fig:watts-strogatz}
\end{figure}

When $n>2k$, there are in total $nk$ edges in the network, which remain constant throughout the rewiring process.

\begin{remark}
    Instead of a ring in with $n$ equal spaced vertices $\real^2$, we can also start with a torus in higher dimensions. For example, in $\real^3$, we fold a square lattice of $n^2$ vertices into a torus.
\end{remark}  

One of the most popular modifications of Watts-Strogatz is the \textbf{Newman-Moore-Watts} model, where instead of rewiring, we introduce random shortcuts:
\begin{itemize}
    \item Start with a ring of $n$ vertices, where each vertex is connected to its $k$ nearest neighbours.
    \item Add shortcut between $v$ and $u\notin \text{bd}(v)$ with probability $p$.
    \item Repeat for some iterations.
\end{itemize}

This gives an exact degree distribution with $d(v) \sim 2k+\mathrm{Binom}(n-2k-1, p)$.
\begin{unexaminable}
    To see this, note every vertex starts with $2k$ edges. Then, we choose from the $n-2k-1$ vertices to connect to it with probability $p$.
\end{unexaminable}

However, the degree distribution of real-world networks often follows a power law, i.e. $p(d)\sim d^{-\gamma}$ for some $\gamma>0$.  

\begin{remark}
    Intuitively, the power-law distribution says that vertices tend to attach to "popular" vertices.
\end{remark}

\subsection{Barab\'{a}si-Albert Model}

\begin{definition}
    A probability measure with p.d.f. $p(d)$ is said to follow a \textbf{power law} if there exists $\gamma>0$ and some constant $C>0$ such that $p(d)\sim C d^{-\gamma}$ as $d\to \infty$, where $\gamma$ is called the \textbf{power-law exponent}.
\end{definition}

To explain this power-law behaviour, Albert and Barab\'{a}si proposed the \textbf{preferential attachment model}, which has the following construction:
\begin{itemize}
    \item Start with $m_0$ vertices, the edges between which are formed arbitrarily.  
    \item (Growth) At each time $t$, add a new vertex with $m\leq m_0$ edges that connect to $m$ existing vertices.
    \item (Preferential attachment) The probability that the new vertex connects to vertex $v_i$ is given by:
    \begin{equation*}
        \pi_i = \frac{d(v_i)}{\sum_j d(v_j)}
    \end{equation*}
\end{itemize}

The resulting graph may contain self-loops and multi-edges. A more mathematically rigorous way to define this model is given in \citep{barabasi2016network}.  

\begin{unexaminable}
    To see why the distribution of degrees follows a power law, we consider a continuous-time approximation to the degree of vertex $v_i$, denoted by $k_i(t)$. Then the rate of change is given by:
    \begin{equation*}
        \frac{d k_i}{dt} = m \frac{k_i}{\sum_j k_j}
    \end{equation*}
    But noting that $\sum_j k_j = 2mt - m$, we can solve this equation  to obtain:
    \begin{equation*}
        k_i(t) = m \left( \frac{t}{t_i} \right)^{\frac{1}{2}}
    \end{equation*}

    where $t_i$ is the time when vertex $v_i$ is added (we used the initial condition $k_i(t_i)=m$).

    To compute the degree distribution, note $\mathbb{P}(k_i(t)>k) = \mathbb{P}(t_i < \frac{m^2}{k^2} t) = \frac{m^2}{k^2}$, if we assume the vertices were added uniformly over time: $t_i \sim \mathrm{Unif}(0,t)$. Now differentiating with respect to $k$, we obtain:
    \begin{equation*}
        \mathbb{P}(d(v_i)=k) \approx 2m^2 k^{-3}
    \end{equation*}
    for $k$ large. This is a power law with $\gamma=3$.
\end{unexaminable}

\subsection{Stochastic Block Model}

This is also called Erd\H{o}s-R\'{e}nyi mixture model. The graph is constructed as follows:
\begin{itemize}
    \item Start with $n$ vertices, which are partitioned into $L$ blocks $V_1,\ldots,V_L$.
    \item Edges are formed independently between vertices in different blocks with probability $p_{ij}$.
\end{itemize}

\begin{proposition}
    For a random vertex $V$, the expected degree is given by:
    \begin{equation*}
        \mathbb{E}[d(V)] = \sum_{\ell=1}^L \alpha_{\ell}\left((|\mathcal{V}|\alpha_{\ell}-1)p_{\ell,\ell}+\sum_{k\ne\ell}|\mathcal{V}|\alpha_{k}p_{k,\ell}\right)
    \end{equation*}
    where $\alpha_{\ell} = \frac{|V_{\ell}|}{|\mathcal{V}|}$ is the proportion of vertices in block $\ell$.
\end{proposition}
\begin{proof}
    Since we have chosen the vertex $V$ uniformly, we can condition on the particular vertex $V=v_i$:
    \begin{align*}
        \mathbb{E}[d(V)] &= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[d(V)\mid V=v_i]\\
        &= \frac{1}{n} \sum_{i=1}^n \sum_{\ell=1}^L \mathbf{1}(v_i \in V_{\ell}) \sum_{j=1, j\neq i}^n \mathbb{P}(v_i \sim v_j)
    \end{align*}

    The inner sum is given by:
    \begin{align*}
        \sum_{j=1, j\neq i}^n \mathbb{P}(v_i \sim v_j) &= \sum_{j=1, j\neq i}^n \sum_{k=1}^L \mathbf{1}(v_j \in V_k) \mathbb{P}(v_i \sim v_j)\\
        &= \sum_{j=1, j\neq i}^n \left[\sum_{k=1, k\neq \ell}^L \mathbf{1}(v_j \in V_k) \mathbb{P}(v_i \sim v_j) + \mathbf{1}(v_j \in V_\ell) \mathbb{P}(v_i \sim v_j)\right]\\
        &= \sum_{j=1, j\neq i}^n \left[\sum_{k=1, k\neq \ell}^L \mathbf{1}(v_j \in V_k) p_{k,\ell} + \mathbf{1}(v_j \in V_\ell) p_{\ell, \ell}\right]\\
        &= \sum_{k=1, k\neq \ell}^L p_{k,\ell} \left[\sum_{j=1, j\neq i}^n \mathbf{1}(v_j \in V_k)\right]  +p_{\ell, \ell} \sum_{j=1, j\neq i}^n \mathbf{1}(v_j \in V_\ell) \\
        &= \sum_{k=1, k\neq \ell}^L p_{k,\ell} |V_k| + p_{\ell, \ell} (|V_\ell|-1)
    \end{align*}

    Now combining the two equations, we obtain the result:
    \begin{align*}
        \mathbb{E}[d(V)] &= \frac{1}{n} \sum_{i=1}^n \sum_{\ell=1}^L \mathbf{1}(v_i \in V_{\ell}) \left[\sum_{k=1, k\neq \ell}^L p_{k,\ell} |V_k| + p_{\ell, \ell} (|V_\ell|-1)\right]\\
        &= \sum_{\ell=1}^L \left[\frac{1}{n} \sum_{i=1}^{n} \mathbf{1}(v_i \in V_{\ell})\right] \left[\sum_{k=1, k\neq \ell}^L p_{k,\ell} |V_k| + p_{\ell, \ell} (|V_\ell|-1)\right]\\
        &= \sum_{\ell=1}^{L} \alpha_\ell \left[\sum_{k=1, k\neq \ell}^L p_{k,\ell} |V_k| + p_{\ell, \ell} (|V_\ell|-1)\right]
    \end{align*}
    as required.  
\end{proof}


A variant of this model is to assign each vertex a block at random with:    

\begin{equation*}
    \mathbb{P}(v\in V_{\ell}) = \alpha_{\ell}
\end{equation*}  

\begin{proposition}
    For a random vertex $V$, the expected degree is given by:
    \begin{equation*}
        \mathbb{E}[d(V)] = (|V|-1)\sum_{\ell=1}^{L}\sum_{k=1}^{L}\alpha_{\ell}\alpha_{k}p_{k,\ell}
    \end{equation*}
    for probabilities $\alpha_{\ell}$ and $p_{k,\ell}$.
\end{proposition}

\begin{proof}
    We proceed similarly to the previous proof:
    \begin{align*}
        \mathbb{E}[d(V)] &= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[d(V)\mid V=v_i]\\
        &= \frac{1}{n} \sum_{i=1}^n \sum_{\ell=1}^L \mathbb{P}(v_i \in V_{\ell}) \sum_{j=1, j\neq i}^n \mathbb{P}(v_i \sim v_j)\\
        &= \frac{1}{n} \sum_{i=1}^n \sum_{\ell=1}^L \alpha_\ell \sum_{j=1, j\neq i}^n \mathbb{P}(v_i \sim v_j)
    \end{align*}
    The inner sum is given by:
    \begin{align*}
        \sum_{j=1, j\neq i}^n \mathbb{P}(v_i \sim v_j) &= \sum_{j=1, j\neq i}^n \sum_{k=1}^L \mathbb{P}(v_j \in V_k) \mathbb{P}(v_i \sim v_j)\\
        &= \sum_{j=1, j\neq i}^n \left[\sum_{k=1}^L \mathbb{P}(v_j \in V_k) \mathbb{P}(v_i \sim v_j)\right]\\
        &= \sum_{j=1, j\neq i}^n \left[\sum_{k=1}^L \alpha_k p_{k,\ell}\right]\\
        &= (n-1) \sum_{k=1}^L \alpha_k p_{k,\ell}
    \end{align*}

    Now combining the two equations, we obtain the result:
    \begin{align*}
        \mathbb{E}[d(V)] &=\frac{1}{n} \sum_{i=1}^n \sum_{\ell=1}^L \alpha_\ell \sum_{j=1, j\neq i}^n \mathbb{P}(v_i \sim v_j) \\
        &= \frac{1}{n} \sum_{i=1}^n \sum_{\ell=1}^L \alpha_\ell (n-1) \sum_{k=1}^L \alpha_k p_{k,\ell} \\
        &= (n-1) \sum_{\ell=1}^L \sum_{k=1}^L \alpha_\ell \alpha_k p_{k,\ell} \\
        &= (|V|-1) \sum_{\ell=1}^L \sum_{k=1}^L \alpha_\ell \alpha_k p_{k,\ell}
    \end{align*}
    as required (here $n=|V|$).
\end{proof}

%  which is computed similarly $\mathbb{E}[d(v)]=\mathbb{E}[\mathbb{E}[d(v) \mid v \in V_i]]=\sum_{\ell=1}^{L}\alpha_{\ell}\left(\sum_{k}(|\mathcal{V}|-1)\alpha_{k} p_{k, \ell}\right)$, since we do not know how many vertices are in each block, so $|\mathcal{V}|\alpha_{\ell}$ disappears.



\subsection{The Configuration Model}  
Given a feasible degree sequence $(d(i))_{1\leq i \leq n}$, the configuration model constructs a random graph as follows: 
\begin{itemize}
    \item For each vertex $i$, create $d(i)$ half-edges (stubs).
    \item Choose two half-edges uniformly at random and connect them.
    \item Repeat until all half-edges are connected.
\end{itemize}

\paragraph{Multi-edges} Here we assume that all degrees are at least $1$ and $\sum_i d(i) = 2m$. The expected number of edges between vertices $i$ and $j$ is given by:
\begin{equation*}
    \frac{d(i)d(j)}{2m-1}
\end{equation*}

as each of the half-edge of $i$ can choose any of the $d(j)$ half-edges of $j$ with probability $\frac{1}{2m-1}$.  

\paragraph{Self-loops} For vertex $i$, the expected number of self-loops for vertex $i$ is given by:
\begin{equation*}
    \frac{d(i) (d(i)-1)}{2(2m-1)}
\end{equation*}
since there are $d(i)$ half-edges of $i$ and each of them can choose any of the $d(i)-1$ half-edges of $i$ with probability $\frac{1}{2m-1}$; the number is divided by $2$ since each edge is counted twice.  

Hence the total expected number of self-loops is:
\begin{equation*}
    \sum_{i=1}^{n}\frac{d(i)(d(i)-1)}{2(2m-1)}=\frac{\mathbb{E}[d(V)^{2}]-\mathbb{E}[d(V)]}{2(\mathbb{E}[d(V)]-1/n)}
\end{equation*}

where $d(V)$ is the degree of a randomly chosen vertex and 
\begin{align*}
    \mathbb{E}[d(V)] &= \frac{1}{n} \sum_{i=1}^{n} d(i) = \frac{2m}{n} \\
    \mathbb{E}[d(V)^{2}] &= \frac{1}{n} \sum_{i=1}^{n} d(i)^{2}
\end{align*}

It follows that if $\mathbb{E}[d(V)^{2}]$ is bounded, then the expected number of self-loops remains bounded, whereas the number of edges grows. Hence, self-loops are negligible in the limit. A similar argument can be made for multi-edges.  

\begin{unexaminable}
    The validity of the configuration model is mentioned in Part C of PS 1, which discusses the Havel-Hakimi algorithm. The following content is not examinable.
\end{unexaminable}

\begin{definition}
    A sequence is said to be \textbf{graphical} if it is the degree sequence of some graph.
\end{definition}

The Havel-Hakimi algorithm constructs a graph with a given degree sequence.
\begin{theorem}[Havel-Hakimi algorithm]
    Let $A=(s, t_1, \ldots, t_s, d_1, \ldots, d_n)$ be a sequence of non-negative integers in non-increasing order. Then $A$ is graphical if and only if $A'=(t_1-1, t_2-1, \ldots, t_s-1, d_1, \ldots, d_s)$ is graphical.
\end{theorem}
\begin{proof}[Sketch of proof]
    The proof is by induction on $n$. The base case is trivial. We consider two cases: 
    \begin{enumerate}
        \item If all $s$ vertices of degree $t_i$ are adjacent to the first, then we remove the first vertex $S$ and subtract $1$ from the first $s$ entries of $A$. Done by induction.
        \item Otherwise, $S$ is adjacent to some $D_j$ with degree $d_j \leq t_i$. If $d_j=t_i$, switch their places and the degree sequence remains unchanged. If $t_i>d_j$, then $\exists W \sim T_i$, but $W \nsim D_j$. So we can delete the edge $S-D_j$ and $T_i-W$ and add the edges $S-T_i$ and $D_j-W$. The degree sequence remains unchanged. So we can repeat this to reduce the problem to case 1.
    \end{enumerate}
\end{proof}


\subsubsection{Chung-Lu Model}

The Chung-Lu model (also called the Newman-Girvan model) is a variant of the configuration model. We start with weights of non-negative integers $\{w_i\}_{1\leq i \leq n}$ and construct a random graph as follows:
\begin{itemize}
    \item Compute $\theta_i = \frac{w_i}{\sqrt{\sum_j w_j}}$ for $1\leq i \leq n$.
    \item Connect two vertices $i$ and $j$ with probability $\min \{\theta_i \theta_j, 1\}$ independently of all other edges.
\end{itemize}

Often we assume $\max_i w_i^2 < \sum_k w_k$ so that $\theta_i < 1$ for all $i$ (otherwise the probability of an edge might be $1$).  

The expected degree of vertex $i$ is given by:
\begin{align*}
    \mathbb{E}[d(i)] &= \sum_{j=1}^{n} \mathbb{P}(i \sim j) = \sum_{j=1}^{n} \min \{\theta_i \theta_j, 1\} \\
    &= \sum_{j=1}^{n} \min \{\frac{w_iw_j}{\sum_k w_k}, 1\} = \frac{w_i}{\sum_k w_k} \sum_{j=1}^{n} w_j = w_i
\end{align*}

where we used the assumption $\max_i w_i^2 < \sum_k w_k \implies w_iw_j < \max w_i^2 < \sum_k w_k$. 

\subsection{Geometric Random Graphs}
In this model, we assume that the vertices are points in $\real^d$ and the probability of an edge between two vertices is a function of their distance. Namely, we construct as follows:
\begin{itemize}
    \item Choose distribution $\mu$ on $\real^d$.
    \item Vertices $X_1,\ldots,X_n$ are i.i.d. samples from $\mu$.
    \item Choose value $r>0$.
    \item Connect vertices $i$ and $j$ if $\|X_i-X_j\|_2\leq r$.
\end{itemize}


\subsection{Exponential Random Graph Models}
Exponential random graphs incorporate covariates into the probability of an edge. We model the whole adjacency matrix $\mathbf{X}$ as a random variable with the following distribution:
\begin{equation*}
    \mathbb{P}(\mathbf{X}=\mathbf{A}) = \frac{\exp(\theta^T \mathbf{z}(\mathbf{A}))}{Z(\theta)}
\end{equation*}

where $\theta$ is a vector of parameters, $\mathbf{z}(\mathbf{A})$ is a vector of statistics of $\mathbf{A}$, and $Z(\theta)$ is the normalising constant. 


\newpage
\bibliographystyle{apalike}
\bibliography{bibliography2.bib}




\end{document}