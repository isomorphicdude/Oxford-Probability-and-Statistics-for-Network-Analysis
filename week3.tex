\documentclass{article}

\input{includes/commands.tex}
\input{includes/theoremstyle.tex}
\usepackage{tikz}
\usepackage[colon, sort&compress]{natbib}
% \pagestyle{fancy}

\title{Week 3 \& 4 Stein's Method for Graphs}

\date{\today}

\begin{document}
% \author{\aut}
\maketitle

\section{Stein's Method for Graphs}

\subsection{Motivation}
% \begin{unexaminable}
%     Ask \verb|ChatGPT| for better motivations.
% \end{unexaminable}
Consider the problem of counting the number of triangles in the Erd\H{o}s-R\'{e}nyi random graph $\mathcal{G}(n,p)$. We know that the expected number of triangles is $\binom{n}{3}p^3$. To obtain the (approximate) distribution of the number of triangles, the central limit theorem is not applicable here, as the number of triangles is not a sum of \textit{independent} random variables:  

\begin{equation*}
    T=\sum_{i<j<k} A_{ij}A_{jk}A_{ki}
\end{equation*}

Stein's Method allows us to approximate the distribution of $T$ by another known distribution and provides a way to bound the error.  

We introduce the general idea of Stein's Method and then consider the cases of a Poisson approximation and a normal approximation.

\subsection{Basic Ingredients}

\begin{unexaminable}
    This section is not examinable. The content of this section is based on \citep{anastasiou2022steins}.
\end{unexaminable}

Main ideas:   
\begin{itemize}
    \item Task is to determine how close a random variable $Z \sim Q$ is to a target distribution $P$.
    \item Can show $\mathbb{E}[(\mathcal{T}f (Z))]=0, \forall f \iff Z\sim P$. (Think of $\mathcal{T}f$ as a test) 
    \item  In particular, $Z$ is approximately $P$-distributed if $\mathbb{E}[(\mathcal{T}f (Z))]\approx 0$ for all $f$.  
\end{itemize}

More formally, we set up Stein's method for a target probability measure $P$ and any other measure $Q$. Let $\mathcal{G}(\mathcal{T})$ be a set of functions determined by a linear operator $\mathcal{T}$.  

\begin{definition}\label{def:stein_operator}
    The \textbf{Stein operator} $\mathcal{T}$ is a linear operator such that
    \begin{center}
        $P=Q$ if and only if $\mathbb{E}_{Z\sim Q}[(\mathcal{T}g)(Z)]=0$ for all $g \in \mathcal{G}(\mathcal{T})$.
    \end{center}
    The set of functions $\mathcal{G}(\mathcal{T})$ such that $\mathbb{E}_{Z \sim P}[(\mathcal{T}g)(Z)]=0, \forall g \in \mathcal{G}(\mathcal{T})$ is called the \textbf{Stein class} of $\mathcal{T}$.
\end{definition}

The equation $\mathbb{E}_{Z\sim Q}[(\mathcal{T}g)(Z)]=0$ is called the \textbf{Stein identity}.  

With this setup, we can define how close is $Z$ a $P$-distributed using the characterization above:

\begin{definition}
    The \textbf{Stein discrepancy} between $P$ and $Q$ is defined as\label{def:stein_discrepancy}
    \begin{equation*}
        \mathcal{D}(Q, \mathcal{T}, \mathcal{G})=\sup_{g \in \mathcal{G}(\mathcal{T})} \|\mathbb{E}_{Z\sim Q}[(\mathcal{T}g)(Z)]\|^*
    \end{equation*}
    for some norm $\|\cdot\|^*$.
\end{definition}

The closer this discrepancy is to zero, the closer $Q$ is to $P$. 

Usually, this discrepancy is determined by various types of integral probability metrics (IPMs).

\begin{definition}\label{def:ipm}
    An \textbf{Integral Probability Metric (IPM)} is a function $d_{\mathcal{H}}$, s.t. 
    \begin{equation*}
        d_\mathcal{H}(P, Q) = \sup_{h \in \mathcal{H}} |\mathbb{E}_{X\sim P}[h(X)] - \mathbb{E}_{Z\sim Q}[h(Z)]|
    \end{equation*}
    The set $\mathcal{H} \subset L^1(P) \cap L^1(Q)$ is the class of test functions; if such function $d_\mathcal{H}$ is a metric, then $\mathcal{H}$ is called \textbf{measure determining}.
\end{definition}

An important example is the total variation distance, which we will use in the next section.  

\begin{example}
    The \textbf{total variation distance} is an IPM with $\mathcal{H}=\{1_A: A \in \mathcal{F}\}$, where $\mathcal{F}$ is the $\sigma$-algebra of events, which admits test functions $h(x) = \mathbf{1}(x\in A)$. The total variation distance is defined as  
    \begin{equation}
        d_{TV}(P, Q) = \sup_{A \in \mathcal{F}} |P(A) - Q(A)|
        \label{eq:total_variation_distance}
    \end{equation}
    It also has an alternative representation with $\mathcal{H} = \{f: \|f\|_\infty \leq 1\}$:
    \begin{equation*}
        d_{TV}(P, Q) = \frac{1}{2} \sup_{f \in \mathcal{H}}\left|\int f dP - \int f dQ\right|
    \end{equation*}
    A similar representation exists when the distributions admit Radon-Nikodym derivatives $\frac{dP}{d\mu}=p$ and $\frac{dQ}{d\mu}=q$ with respect to a $\sigma$-finite measure $\mu$:
    \begin{equation*}
        d_{TV}(P, Q) = \frac{1}{2} \int |p - q| d\mu
    \end{equation*}
\end{example}

The proof for the second representation can be found \href{https://math.stackexchange.com/questions/3287889/show-that-the-total-variation-distance-of-probability-measures-mu-nu-is-equa}{here}, and the third can be found \href{https://math.stackexchange.com/questions/1481101/definition-of-the-total-variation-distance-vp-q-frac12-int-p-qd-n}{here}.  

The concept of Stein discrepancy and general IPMs are related by Stein's equation.

\begin{definition}\label{def:stein_equation}
    The \textbf{Stein's equation} for $h\in \mathcal{H}$ is a functional equation:
    \begin{equation*}
        \mathcal{T}g (z) = h(z) - \mathbb{E}_{X\sim P}[h(X)]
    \end{equation*}
    evaluated over $z$ on the support of $P$, where $g=g(h)$ is a solution to the Stein equation.
\end{definition}

If the solution $g$ exists, then we can take the expectation over $Q$ to get:
\begin{align*}
    \mathbb{E}_{Z\sim Q}[(\mathcal{T}g)(Z)] &= \mathbb{E}_{Z\sim Q}[h(Z) - \mathbb{E}_{X\sim P}[h(X)]]\\
    &= \mathbb{E}_{Z\sim Q}[h(Z)] - \mathbb{E}_{X\sim P}[h(X)]
\end{align*}

Now taking the supremum over $h \in \mathcal{H}$, we recover the form of an IPM, that is,

\begin{equation}\label{eq:stein_ipm_relation}
    d_{\mathcal{H}}(P, Q) = \sup_{h \in \mathcal{H}} |\mathbb{E}_{X\sim P}[h(X)] - \mathbb{E}_{Z\sim Q}[h(Z)]| = \sup_{g \in \mathcal{G}(\mathcal{T})} |\mathbb{E}_{Z\sim Q}[(\mathcal{T}g)(Z)]|
\end{equation}

\begin{remark}\label{rem:stein_ipm_relation}
    Some comments on \Cref{eq:stein_ipm_relation}:
    \begin{itemize}
        \item The information about $P$ is encoded in the linear operator $\mathcal{T}$ and function $g$:
        \begin{align*}
            \mathcal{T}(g)(w) &= \lambda g(w+1) - wg(w) \quad \text{(Poisson approximation)}\\
            \mathcal{T}(g)(w) &= g'(w) - wg(w) \quad \text{(Normal approximation)}
        \end{align*}
        \item By choosing $\mathcal{H}$ in Stein's equation, we can choose the IPM or any convergence:
        \begin{align*}
            \mathcal{H} &= \{1_A: A \in \mathcal{F}\} \quad \text{(total variation distance)}\\
            \mathcal{H} &= \{h: h \text{ is bounded, continuous a.s., piecewise } C^1\} \quad \text{(weak convergence)}
        \end{align*}
        \item Bounding the last term is often easier, as it only requires taking expectations over $Q$:
        \begin{align*}
            d_{TV}(P, Q) = \sup_{A \in \mathcal{F}} |P(A) - Q(A)|&\leq \sup_{g \in \mathcal{G}(\mathcal{T})} \|\mathbb{E}_{W\sim Q}[\lambda g(W+1) - Wg(W)]|
             \\
            |\mathbb{E}_Q[h(W)] - \mathbb{E}_P[h(X)]| &\leq \sup_{g \in \mathcal{G}(\mathcal{T})} |\mathbb{E}_{W\sim Q}[g'(W) - Wg(W)]|
        \end{align*}
        where the first equation is the Stein-Chen method for Poisson approximation and the second is for Stein's normal approximation.
    \end{itemize}
\end{remark}



\begin{unexaminable}
    For an interesting application in Machine Learning, see \citep{pmlr-v119-grathwohl20a}.
\end{unexaminable}


% \newpage
\subsection{Stein-Chen Method for Poisson Approximation}  
Following the framework in the previous section, we can define the Stein operator (\Cref{def:stein_operator}) for Poisson approximation\footnote{We use the same notations as in the lecture notes, denoting the random variable as $W$ instead of $Z$.}:
\begin{equation}
    \mathcal{T}g(w) = \lambda g(w+1) - wg(w)
\end{equation}

for $g: \mathbb{N}_0 \to \mathbb{R}$. This gives a characterisation of the Poisson distribution with parameter $\lambda>0$:

\begin{proposition}
    \label{prop: direction1 poisson}
    If $W\sim \text{Po}(\lambda)$, then $\mathbb{E}[(\mathcal{T}g)(W)]=0$ for all $g$ bounded.
\end{proposition}

\begin{proof}
This is a direct computation, noting the $W=0$ gives zero expectation when $g$ is bounded:  
    \begin{align*}
        \lambda \mathbb{E}[g(W+1)] &= \lambda \sum_{k=0}^\infty g(k+1) \frac{\lambda^k}{k!} e^{-\lambda}\\
        &= \lambda \sum_{k=1}^\infty g(k) \frac{\lambda^{k-1}}{(k-1)!} e^{-\lambda} \quad \text{(move out $\lambda$)}\\
        &= \sum_{k=1}^\infty g(k) \frac{\lambda^k}{(k-1)!} e^{-\lambda} \quad \text{(shift index)}
        \\
        &= \sum_{k=1}^\infty g(k) \frac{\lambda^k}{k!} e^{-\lambda} \cdot k \quad \text{(divide and multiply by $k$)}\\
        &= \mathbb{E}[Wg(W)]
    \end{align*}
So we have shown $\lambda\mathbb{E}[g(W+1)] = \mathbb{E}[Wg(W)]$ for all bounded $g$, which implies $\mathbb{E}[(\mathcal{T}g)(W)]=0$.  
\end{proof}


The IPM we will use here is the total variation distance (\ref{eq:total_variation_distance}), which leads to Stein's equation (\Cref{def:stein_equation}):

\begin{equation}
    \mathcal{T}g(w) = \mathbf{1}(w \in A) - \mathbb{E}[\mathbf{1}(X \in A)]
    \label{eq:poisson_stein_equation}
\end{equation}

We now claim this equation has a \textit{unique} solution.

\begin{lemma}\label{lem:poisson_stein_equation}
    Given \Cref{eq:poisson_stein_equation}, we have a unique solution
    \begin{equation}
        g(w+1) = \frac{w!}{\lambda^{w+1}} e^\lambda \sum_{k=0}^w \frac{\lambda^k}{k!} (\mathbf{1}(k\in A) - \mathbb{E}[\mathbf{1}(k\in A)])
    \end{equation}

    this solution can also be written as 
    \begin{equation}
        g(w+1) = -\frac{w!}{\lambda^{w+1}} e^\lambda \sum_{k=w+1}^\infty \frac{\lambda^k}{k!} (\mathbf{1}(k\in A) - \mathbb{E}[\mathbf{1}(k\in A)])
    \end{equation}
\end{lemma}

\begin{proof}
    We first show the claimed solution satisfies Stein's equation, which is easily verified by direct computation.  
    
    To show uniqueness, let $w=0$ in \Cref{eq:poisson_stein_equation} and we get $\lambda g(1) = \mathbf{1}(0\in A) - \mathbb{E}[\mathbf{1}(0\in A)]$. So if $f$ is another solution to \Cref{eq:poisson_stein_equation}, then $f(1)=g(1)$. Now we can use induction to show $f(w)=g(w)$ for all $w\in \mathbb{N}_0$.
\end{proof}

To bound the total variation distance, we require additional bounds on $g$.  
\begin{remark}
    In fact, as we shall see in the proof below, only those bounds and properties of $g$ are used; the explicit form of $g$ is unimportant
\end{remark}

\begin{lemma}\label{lem:poisson_stein_bound}
For the solution $g$ to the Stein's equation \Cref{eq:poisson_stein_equation}, we have:
\begin{equation}
    \sup_{k \in \mathbb{N}_0} |g(k)| \leq \min (1, \lambda^{-1/2})
    \label{eq:poisson_stein_bound1}
\end{equation}
and 
\begin{equation}
    \sup_{k \in \mathbb{N}_0} |g(k+1) - g(k)| \leq \min (1, \lambda^{-1})
    \label{eq:poisson_stein_bound2}
\end{equation}

Here $\lambda^{-1}$ is referred to as the \textit{magic factor}.
\end{lemma}

\begin{proof}
    The proof is omitted in the lecture notes, which is not examinable. 
\end{proof}

Now we give the converse of \Cref{prop: direction1 poisson}.

\begin{proposition}\label{prop: direction2 poisson}
    If $\mathbb{E}[(\mathcal{T}g)(W)]=0$ for all $g$ bounded, then $W\sim \text{Po}(\lambda)$.
\end{proposition}

\begin{proof}
    Since $g$ is an arbitrary bounded function, we take it as the solution obtained in \Cref{lem:poisson_stein_equation}, which is bounded by the previous lemma. Then taking the expectation of \Cref{eq:poisson_stein_equation} finishes the proof.
\end{proof}

Now we can bound the total variation distance between $W$ and $\text{Po}(\lambda)$:
\begin{equation}\label{eq:poisson_stein_total_variation}
    d_{TV}(\mathcal{L}(W), \text{Po}(\lambda)) = \sup_{g} |\mathbb{E}[\lambda g(W+1) - Wg(W)]|
\end{equation}

As an example, we consider a bound for Bernoulli random variables. 

\myparagraph{Example: Sum of i.i.d. Bernoulli random variables}  

    Let $X_1, \ldots, X_n$ be independent Bernoulli random variables, where $X_i \sim \text{Ber}(p_i)$ and $W = \sum_{i=1}^n X_i$. Setting $\lambda = \sum_{i=1}^n p_i$, the Stein's equation is given by $\mathbb{E}[\lambda g(W+1) - Wg(W)]$.    

    We first compute $\mathbb{E}[Wg(W)]$:
    \begin{align*}
        \mathbb{E}[Wg(W)] &= \mathbb{E}[\sum_{i=1}^n X_i g(W)]\\
        &= \sum_{i=1}^n \mathbb{E}[X_i g(W - X_i + 1)] \quad \text{(since $X_i$ is binary)}
        \\
        &= \sum_{i=1}^n \mathbb{E}[X_i] \mathbb{E}[g(W - X_i + 1)] \quad \text{(by independence)}\\
        &= \sum_{i=1}^n p_i \mathbb{E}[g(W - X_i + 1)]
    \end{align*}

    where in the second step we use the fact that $X_i=0$ when $g$ is bounded yields wero expectation. Thus, using the results above with $\mathbb{E}[Wg(W)] =  \sum_{i=1}^n p_i \mathbb{E}[g(W - X_i + 1)]$, we have:  
    \begin{align*}
        \mathbb{E}[\lambda g(W+1) - Wg(W)] &= \sum_{i=1}^n p_i \mathbb{E}[g(W + 1)] - \sum_{i=1}^n p_i \mathbb{E}[g(W - X_i + 1)]\\
        &= \sum_{i=1}^n p_i \mathbb{E}[\mathbb{E}[g(W+1) - g(W - X_i + 1) \mid X_i]] \ \text{(using $\mathbb{E}[\mathbb{E}[Y|X_i]]=\mathbb{E}[Y]$)}\\
        &\stackrel{\textbf{A}}{=} \sum_{i=1}^n p_i^2 \mathbb{E}[g(W+1) - g(W) \mid X_i=1] \\
        & \leq \sum_{i=1}^n p_i^2 \mathbb{E}[|g(W+1) - g(W)| \mid X_i=1] \ \text{(using $\mathbb{E}[Y|X_i]\leq\mathbb{E}[|Y||X_i]$)}
    \end{align*}
    where  in step $\textbf{A}$ we used the fact that 
    \[
    \mathbb{E}[\mathbb{E}[Y|X_i]]=p_i \mathbb{E}[Y|X_i=1] + (1-p_i) \mathbb{E}[Y|X_i=0]
    \]
    Applying \Cref{lem:poisson_stein_bound} now gives\footnote{Here the conditional expectation is not a random variable with $X_i$'s value specified. See \citep{durrett2010probability}.}
    \begin{align*}
        |\mathbb{E}[\lambda g(W+1) - Wg(W)]| &\leq \left|\sum_{i=1}^n p_i^2 \mathbb{E}[|g(W+1) - g(W)| \mid X_i=1]\right|\\
        &\leq \left|\sum_{i=1}^n p_i^2 \mathbb{E}[\min (1, \lambda^{-1})\mid X_i=1]\right|\\
        &\leq \min (1, \lambda^{-1}) \sum_{i=1}^n p_i^2
    \end{align*}
    Using \Cref{eq:poisson_stein_total_variation}, the total variation distance is bounded by
    \begin{equation*}
        d_{TV}(\mathcal{L}(W), \text{Po}(\lambda)) \leq \min (1, \lambda^{-1}) \sum_{i=1}^n p_i^2
    \end{equation*}
    Note this bound is non-asymptotic and holds for any $n$.

\subsubsection{Local dependence}  
When the random variables are not independent, the computation of $\mathbb{E}[Wg(W)]$ is more complicated.  Here we let $A_\alpha$ denote the set elements that are dependent on $X_\alpha$.  

\begin{theorem}\label{thm:poisson_stein_local}
    Let $X_\alpha, \alpha \in I$ with each $X_\alpha \sim \text{Ber}(p_\alpha)$ and $W = \sum_{\alpha \in I} X_\alpha$.  
    Suppose $\forall \alpha \in I$, there exists a set $A_\alpha \subseteq I$, such that
    \begin{center}
        $X_\alpha$ is independent of $\sum_{\beta \notin A_\alpha} X_\beta$. 
    \end{center}
    Define $W_\alpha = W - \eta_\alpha$, where
    \begin{equation*}
        \eta_\alpha = \sum_{\beta \in A_\alpha} X_\beta
    \end{equation*}  
    
    Then the total variation distance is bounded by
    \begin{equation*}
        d_{TV}(\mathcal{L}(W), \text{Po}(\lambda)) \leq \sum_{\alpha\in I}[(p_{\alpha}\mathbb{E}(\eta_{\alpha})+\mathbb{E}(X_{\alpha}(\eta_{\alpha}-X_{\alpha}))]\operatorname*{min}\left(1,\lambda^{-1}\right)
    \end{equation*}

    where $\lambda = \sum_{\alpha \in I} p_\alpha$.   
\end{theorem}

\begin{proof}

    As before, we aim to bound the total variation distance by bounding $\mathbb{E}|\lambda g(W+1) - Wg(W)|$, where $g$ is the solution to Stein's equation.  
    We compute the term $\mathbb{E}[Wg(W)]$ as follows:  
    \begin{align*}
        \mathbb{E}[Wg(W)] &= \mathbb{E}[\sum_{\alpha \in I} X_\alpha g(W)]\\
        &= \sum_{\alpha \in I} \mathbb{E}[X_\alpha g(W - X_\alpha + 1)] \quad \text{($X_\alpha$ is binary)}
        \\
        &= \sum_{\alpha \in I} \mathbb{E}[X_\alpha g(W_\alpha + \eta_\alpha -X_\alpha +  1)]\quad \text{($W = W_\alpha + \eta_\alpha$)}
        \\
        &= \sum_{\alpha \in I} \mathbb{E}[\textcolor{red}{X_\alpha g(W_\alpha + 1)}] + \mathbb{E}[X_\alpha g(W_\alpha + \eta_\alpha -X_\alpha +  1) - \textcolor{red}{X_\alpha g(W_\alpha + 1)}]\\
        &= \sum_{\alpha \in I} p_\alpha \mathbb{E}[g(W_\alpha + 1)] \\
        &+ \color{blue}{p_\alpha \mathbb{E}[g(W+1)] - p_\alpha \mathbb{E}[g(W+1)]} \\
        &+ \mathbb{E}[X_\alpha (g(W_\alpha + \eta_\alpha -X_\alpha +  1) - g(W_\alpha + 1))]
    \end{align*}
where in the fourth step, we added and subtracted $\mathbb{E}[X_\alpha g(W_\alpha+1)]$ and in the last step we added and subtracted $p_\alpha \mathbb{E}[g(W+1)]$.

Rearranging the terms leads to 
    \[\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(W + 1)] + \underbrace{p_\alpha \mathbb{E}[g(W_\alpha+1) - g(W+1)]}_{R_{1,\alpha}} + \underbrace{\mathbb{E}[X_\alpha (g(W_\alpha + \eta_\alpha -X_\alpha +  1) - g(W_\alpha + 1))]}_{R_{2, \alpha}}\] 

    Hence, let $R_1 = \sum_{\alpha \in I} R_{1,\alpha}$ and $R_2 = \sum_{\alpha \in I} R_{2,\alpha}$,
    the equation we need to bound is
    \begin{equation*}
        \mathbb{E}[\lambda g(W+1) - Wg(W)] =  R_1 + R_2
    \end{equation*}

    For $R_1=\sum_\alpha R_{1,\alpha}$, we have
    \begin{align*}
        |R_1| &= \left|\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(W_\alpha+1) - g(W+1)]\right| \\
        &= \left|\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(W+1) - g(W_\alpha+1)]\right| \\
        &= \left|\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(W_\alpha + \eta_\alpha +1) - g(W_\alpha+1)]\right|\\
        &\leq \sum_{\alpha \in I} p_\alpha |\mathbb{E}[g(W_\alpha + \eta_\alpha +1) - g(W_\alpha+1)]|
    \end{align*}
    Now we apply conditional expectation and telescoping sum to the inner term\footnote{Again, for the sake of clarity, we use conditional expectations of the form $\mathbb{E}[X|Y=m]$.},
    \begin{align*}
        |\mathbb{E}[g(W_\alpha + \eta_\alpha +1) - g(W_\alpha+1)]| &= 
        \left|\mathbb{E}\left[\mathbb{E}[g(W_\alpha + \eta_\alpha +1) - g(W_\alpha+1)\mid \eta_\alpha = m]\right]\right|\\
        &= \left|\mathbb{E}\left[\mathbb{E}[\sum_{k=0}^{m-1} g(W_\alpha + 1 + k+1) - g(W_\alpha + 1 + k)\mid \eta_\alpha = m]\right]\right|\\
        &\leq \mathbb{E}\left[\mathbb{E}[\sum_{k=0}^{m-1} |g(W_\alpha + 1 + k+1) - g(W_\alpha + 1 + k)|\mid \eta_\alpha = m]\right]\\
        &\leq \mathbb{E}\left[\mathbb{E}[\sum_{k=0}^{m-1} \min (1, \lambda^{-1})\mid \eta_\alpha = m]\right]\\
        &= \min (1, \lambda^{-1}) \mathbb{E}[ \eta_\alpha]
    \end{align*}

    where we used that $\sum_{k=0}^{m-1} g(W_\alpha + 1 + k+1) - g(W_\alpha + 1 + k) = g(W_\alpha +m+1)- g(W_\alpha+1)$ for telescoping sum.

    This gives the bound for $R_1$ as:
    \begin{equation*}
        |R_1| \leq \min (1, \lambda^{-1}) \sum_{\alpha \in I} p_\alpha \mathbb{E}[ \eta_\alpha]
    \end{equation*}

    For $R_2=\sum_\alpha R_{2,\alpha}$, we apply the method above to $g(W_\alpha + \eta_\alpha - X_\alpha + 1) - g(W_\alpha + 1)$, conditioning on the value of $\eta_\alpha - X_\alpha$. Namely, we take out the $X_\alpha$ by
    \begin{align*}
        &\mathbb{E}[X_\alpha (g(W_\alpha + \eta_\alpha -X_\alpha +  1) - g(W_\alpha + 1))] \\
        &= \mathbb{E}[\mathbb{E}[X_\alpha (g(W_\alpha + \eta_\alpha -X_\alpha +  1) - g(W_\alpha + 1)) \mid \eta_\alpha - X_\alpha = m]] \\
        &\leq \mathbb{E}[\mathbb{E}[X_\alpha \left(\sum_{k=0}^{m-1} |g(W_\alpha + 1 + k+1) - g(W_\alpha + 1 + k)|\right) \mid \eta_\alpha - X_\alpha = m]]
    \end{align*}
    This gives the bound:
    \begin{equation*}
        |R_2| \leq \min (1, \lambda^{-1}) \sum_{\alpha \in I} p_\alpha \mathbb{E}[ X_\alpha (\eta_\alpha - X_\alpha)]
    \end{equation*}

    Combing those two bounds gives the desired result.
\end{proof}  

We are now ready to approximate the number of triangles in Erd\H{o}s-R\'{e}nyi random graph $\mathcal{G}(n,p)$.   

\medskip

\myparagraph{Example: Number of triangles in Erd\H{o}s-R\'{e}nyi random graph}

We use an index set $\Gamma_n$:

\begin{equation*}
    \Gamma_{n}=\{\alpha=(u,v,w):1\leq u<v<w\leq n\}
\end{equation*}

Then with the same notation as before, let 
$$
X_{\alpha}=X_{u,v,w} = \mathbf{1}(u\sim v) \mathbf{1}(v\sim w) \mathbf{1}(w\sim u)
$$

which equals 1 if and only if $uvw$ is a triangle, each of the $X_\alpha$ is a Bernoulli random variable with parameter $p^3$. 

\begin{theorem}\label{thm:triangle_poisson_dependent}
    Let $W = \sum_{\alpha \in \Gamma_n} X_\alpha$ and $\lambda = \binom{n}{3}p^3$.
    \begin{equation*}
        d_{TV}(\mathcal{L}(W), \text{Po}(\lambda)) \leq \binom{n}{3}p^3(3np^3 + 3np^2) \min (1, \lambda^{-1})
    \end{equation*}
\end{theorem}

\begin{proof}
    Using the same notation as above with $p_\alpha=p^3$ and $A_\alpha$ with $\alpha=(u,v,w)$ be defined as 
    \begin{equation*}
        A_{\alpha} = \{(a,b,c) \mid |(u,v,w)\cap (a,b,c)|\geq 2\}
    \end{equation*}
    It is easy to see $|A_\alpha| = 3(n-3) + 1 < 3n$, since there are  $3(n-3)$ ways to choose triplets with  $|(u,v,w)\cap (a,b,c)| = 2$ exactly\footnote{There are 3 ways to choose the vertex to be different from the original $\alpha$; since we cannot choose the same vertices, there are $n-3$ choices.}, thus $\mathbb{E}[\eta_\alpha]\leq 3np^3$. Moreover, we have  
    \begin{equation*}
        \mathbb{E}[X_\alpha(\eta_\alpha - X_\alpha)] = \mathbb{E}[X_\alpha \sum_{\beta \in A_\alpha, \beta \neq \alpha} X_\beta] = \sum_{\beta \in A_\alpha, \beta \neq \alpha} p^5 \leq 3np^5
    \end{equation*}
    Using \Cref{thm:poisson_stein_local} gives the desired result.
\end{proof}


\subsection{Stein's Method for Normal Approximation}
Similarly, we set up the Stein operator (\Cref{def:stein_operator}) for normal approximation:  

\begin{equation}\label{eq:stein_operator_normal}
    \mathcal{T}g(z) = g'(z) - zg(z)
\end{equation}

for all continuous and piecewise continuously differentiable functions $g: \mathbb{R} \to \mathbb{R}$ with $\mathbb{E}[|g'(W)|]<\infty$. This gives a characterization of the normal distribution with mean $\mu$ and variance $\sigma^2$:  

\begin{theorem}[\citep{stein1972bound}]\label{thm:stein_operator_normal}
    If $W\sim \mathcal{N}(0, 1)$, then 
    \begin{equation*}
        \mathbb{E}[(\mathcal{T}g)(W)]:=\mathbb{E}[Wg(W) - g'(W)] = 0
    \end{equation*}
\end{theorem}

\begin{proof}
    We can compute $\mathbb{E}[g'(W)]$ using integration by parts:
    \begin{align*}
        \mathbb{E}[g'(W)] &= \int_{-\infty}^\infty g'(w) \frac{1}{\sqrt{2\pi}} e^{-w^2/2} dw\\
        &= {\frac{1}{\sqrt{2\pi}}}\int_{-\infty}^{0}g^{\prime}(w)\left(\int_{-\infty}^{w}-x e^{-x^{2}/2}d x\right)\,d w \\
        &+ {\frac{1}{\sqrt{2\pi}}}\int_{0}^{\infty}g^{\prime}(w)\left(\int_{w}^{\infty}x e^{-x^{2}/2}d x\right)\,d w
    \end{align*}

    Now we exchange the order of integration by Fubini's theorem:
    \begin{align*}
        \mathbb{E}[g'(W)] &= \int_{x=-\infty}^0 \left(\int_{w=x}^0 g'(w) dw\right) (-x) e^{-x^2/2} dx + \int_{x=0}^\infty \left(\int_{w=0}^x g'(w) dw\right) xe^{-x^2/2} dx\\
        &={\frac{1}{\sqrt{2\pi}}}\int_{-\infty}^{\infty}[g(x)-g(0)]x e^{-x^{2}/2}d x \\
        &=\mathbb{E}[W g(W)]
    \end{align*}
    as $X$ has mean zero.
\end{proof}

\begin{unexaminable}
    The condition for Fubini can be checked, as $\int_0^\infty |g'(w)| f_W(w) dw \leq \mathbb{E}[|g'(w)|] < \infty$.
\end{unexaminable}

We do not consider the total variation distance here, but instead use the \textbf{weak convergence} of measures, which is motivated by the following example.  

\begin{example}[(Total variation might not be the best choice)]

    Let $X\sim \mathrm{Binom}(n,p)$ and $Y\sim \mathcal{N}(0,1)$, then we know from the central limit theorem that
    \begin{equation*}
        \frac{X-np}{\sqrt{np(1-p)}} \xrightarrow{d} Y
    \end{equation*}
    where $\xrightarrow{d}$ denotes weak convergence. However, if we set $A_n :=\{0, 1, \ldots, n\}$, then $\mathbb{P}(X\in A_n) = 1$ and $\mathbb{P}(Y\in A_n) = 0$, for all $n\in \mathbb{N}$. Thus, no matter how large $n$ is, the total variation distance is always 1, which does not reflect the convergence in distribution.  
\end{example}

We now define the notion of weak convergence of probability measures.  

\begin{definition}
   Let $(P_n)_{n\in \mathbb{N}}$ be a sequence of probability measures on $\mathbb{R}$ and $P$ be another probability measure on $\mathbb{R}$. 
   
   Then \textbf{weak convergence} is equivalent to the following statements:
   \begin{itemize}
    \item $P_n(A) \to P(A)$ for all $A \in \mathcal{B}(\mathbb{R})$ such that $P(\partial A)=0$.
    \item $\int f dP_n \to \int f dP$ for all bounded continuous functions $f$.
    \item $\int f dP_n \to \int f dP$ for all bounded, infinitely-often differentiable continuous functions $f$.
   \end{itemize}
\end{definition}

This motivates the Steins's equation (\Cref{def:stein_equation}):
\begin{equation}\label{eq:stein_equation_normal}
    \mathcal{T}g(z) = h(z) - \mathbb{E}[h(X)]
\end{equation}

where $h$ is a bounded measurable function.


\begin{lemma}\label{lem:stein_equation_normal}
    Given \Cref{eq:stein_equation_normal}, we have a unique bounded solution $g_h$, if:
    \begin{itemize}
        \item $h:\mathbb{R} \to \mathbb{R}$ is bounded and measurable.
        \item $\mathbb{E}[|h(W)|] < \infty$.
        \item $h$ is continuous (a.s.) and piecewise continuously differentiable with $\|h'\|_\infty < \infty$.
    \end{itemize}

        Then the solution can be written as
        \begin{equation*}
            g_h(w) = e^{w^2/2} \int_{-\infty}^w (h(w) - \mathbb{E}[h(X)]) e^{-w^2/2} dw
        \end{equation*}
        which can also be written as
        \begin{equation*}
            g_h(w) = -e^{w^2/2} \int_{w}^\infty (h(w) - \mathbb{E}[h(X)]) e^{-w^2/2} dw
        \end{equation*}
    \end{lemma}

    \begin{proof}
        Multiplying both sides of \Cref{eq:stein_equation_normal} by $e^{-w^2/2}$, we note:
        \begin{align*}
            e^{-w^2/2} g'(w) - e^{-w^2/2}wg(w) &= e^{-w^2/2} (h(w) - \mathbb{E}[h(X)]) 
        \end{align*}
        The LHS is simply the derivative of $e^{-w^2/2}g(w)$, so we can integrate both sides to get:
        \begin{align*}
            e^{-w^2/2}g(w) &= \int_{-\infty}^w e^{-w^2/2} (h(w) - \mathbb{E}[h(X)]) dw +C\\
            g(w) &= e^{w^2/2} \int_{-\infty}^w e^{-w^2/2} (h(w) - \mathbb{E}[h(X)]) dw + Ce^{w^2/2}
        \end{align*}
        Note the solution $g$ is bounded if and only if $C=0$ as otherwise $g\to \infty$ as $w\to \infty$.  
        To obtain the second form, we note that 
        \begin{equation*}
            \int_{-\infty}^{+\infty} (h(w) - \mathbb{E}[h(X)]) e^{-w^2/2} dw = 0
        \end{equation*}
        as $e^{-w^2/2}$ is the density of the zero mean distribution $\mathcal{N}(0,1)$, so the second form follows from symmetry.
    \end{proof}

The bounds on the solution is given by the following lemma.  
\begin{lemma}\label{lem:stein_equation_normal_bound}
    Given $h$ bounded and $g_h$ being its solution in \Cref{lem:stein_equation_normal}, we have:
    \begin{equation*}
        \|g_h\|_\infty \leq \sqrt{\frac{\pi}{2}} \|h - \mathbb{E}[h(X)]\|_\infty
    \end{equation*}
    and 
    \begin{equation*}
        \|g_h'\|_\infty \leq 2 \|h-\mathbb{E}[h(X)]\|_\infty
    \end{equation*}
    For $h$ being continuous and piecewise continuously differentiable, we have:
    \begin{align*}
        \|g_h\|_\infty &\leq 2 \|h'\|_\infty \\
        \|g_h'\|_\infty &\leq \sqrt{\frac{2}{\pi}} \|h'\|_\infty\\
        \|g_h''\|_\infty &\leq 2 \|h'\|_\infty
    \end{align*}
\end{lemma}
\begin{proof}
    The proof is not examinable.
\end{proof}

We can now give the converse of \Cref{thm:stein_operator_normal} to characterize the normal distribution.  

\begin{theorem}[\citep{stein1986}]
    If $\mathbb{E}[(\mathcal{T}g)(W)]=\mathbb{E}[Wg(W) - g'(W)] = 0$ for all bounded $g$, then $W\sim \mathcal{N}(0,1)$.
\end{theorem}

\begin{proof}
    The idea is to take a particular $h$ and use weak convergence. Let $h$ be defined as 
    \begin{equation*}
        h_z(w) = \mathbf{1}_{(-\infty, z]}(w)
    \end{equation*}
    where $z\in \mathbb{R}$. Then it is continuous (a.s.) and piecewise continuously differentiable. Moreover, since the solution $g_h$ is in particular bounded by the previous lemma, we have:  
    \begin{equation*}
        0 = \mathbb{E}_W[Wg_h(W)-g_h'(W)] = \mathbb{E}_W[h(W)] - \mathbb{E}_X[h(X)]=\mathbb{P}(W\leq w) - \Phi(z)
    \end{equation*}
    which implies $W\sim \mathcal{N}(0,1)$ as $w$ is arbitrary.
\end{proof}

Now for independent $X_1, \ldots, X_n$ with zero mean and variance $1/n$, we can bound the weak convergence of their sum to the standard normal distribution.  

\begin{theorem}
    For independent $X_i$, $\mathbb{E}[X_i]=0$ and $\mathbb{E}[X_i^2]=1/n$, let
    $$
    W = \sum_{i=1}^{n} X_i
    $$ 
    Then we have for any continuous and piecewise continuously differentiable $h$:
    \begin{equation*}
        \left|\mathbb{E}[h(W)] - \mathbb{E}[h(X)]\right| \leq \|h'\|_\infty \left( \frac{2}{\sqrt{n}} + \sum_{i=1}^{n} \mathbb{E}[|X_i|^3]\right)
    \end{equation*}
    where $X\sim \mathcal{N}(0,1)$.
\end{theorem}

\begin{proof}
    We would like to bound $\mathbb{E}[g'(W)-Wg(W)]$. Let $W_i = W - X_i$, so $W_i$ is independent of $X_i$. Then we have:
    \begin{align*}
        \mathbb{E}[Wg(W)] &= \mathbb{E}[\sum_{i=1}^{n} X_i g(W)]\\
        &= \sum_{i=1}^{n} \mathbb{E}[X_i g(W_i + X_i)]\\
        &= \sum_{i=1}^{n} \mathbb{E}[X_i (g(W_i) + X_i g'(W_i) + \frac{X_i^2}{2} g''(W_i +\theta X_i))] \ \text{(Taylor expansion at $W_i$)}
        \\
        &= \sum_{i=1}^{n} \mathbb{E}[X_i] \mathbb{E}[g(W_i)] + \mathbb{E}[X_i^2] \mathbb{E}[g'(W_i)] + \frac{1}{2} \mathbb{E}[X_i^3] \mathbb{E}[g''(W_i +\theta X_i)]\\
        &= \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[g'(W_i)] + R \quad \text{($X_i$ has zero mean and unit variance)}
    \end{align*}

    where $R = \sum_{i=1}^{n}\frac{1}{2} \mathbb{E}[X_i^3] \mathbb{E}[g''(W_i +\theta X_i)]$ is the remainder term. 

    Therefore, we have:
    \begin{equation*}
        \mathbb{E}[g'(W)-Wg(W)] = \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[g'(W) -g'(W_i)] - R
    \end{equation*}

    We proceed to bound the two terms.  
    
    For the remainder term $R$, we have:
    \begin{align*}
        |R| &\leq \frac{1}{2} \sum_{i=1}^{n} \mathbb{E}[|X_i|^3] \|g''\|_\infty\\
        &\leq \sum_{i=1}^{n} \mathbb{E}[|X_i|^3] \|h'\|_\infty
    \end{align*}
    

    Again by Taylor expansion or the mean value theorem, we have:
    \begin{align*}
        \left|\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[g'(W) -g'(W_i)]\right| &\leq \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[|W-W_i| \|g''\|_\infty]\\ 
        &\leq \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[|X_i| \|g''\|_\infty] \\
        &\leq \frac{2}{\sqrt{n}} \|h''\|_\infty
    \end{align*}
    where we have used \Cref{lem:stein_equation_normal_bound} and Cauchy-Schwarz inequality in the last step. Namely, 
    \[
        \mathbb{E}[|X_i| \cdot 1]^2 \leq \mathbb{E}[|X_i|^2] \cdot 1=1/n \implies \mathbb{E}[|X_i|]\leq 1/\sqrt{n}
    \] 
    Combining the two bounds using triangle inequality gives the desired result.
\end{proof}

\begin{remark}
    Note here we do not need to specify the distributions of $X_i$ as long as they have zero mean and variance $1/n$, whereas the Poisson approximation in the previous section assumes $X_i$ are Bernoulli random variables.  
\end{remark}

\subsubsection{Dissociated Decompositions}
Again the approximation extends to locally dependent random variables.

\begin{definition}\label{def:dissociated_decomposition}
    A \textbf{dissociated decomposition} of $W = \sum_{i\in I} X_i$ for index $i$ is 
    \begin{align*}
        W &= W_i + Z_i \\
        W_i &= W_{ik} + V_{ik} \quad \text{for $k\in K_i$} 
        % W &= W_{ik} + V_{ik} + Z_i \quad \text{for $k\in K_i$}
    \end{align*}
    where for each $i\in I$, the corresponding $K_i\subseteq I$ satisfies:  
    \begin{itemize}
        \item $W_i$ is independent of $X_i$.
        \item $Z_i=\sum_{k\in K_i} X_k$ is a sum dependent on $X_i$.
        \item $W_{ik}$ is independent of $X_i$ and $X_k$, for $k\in K_i$.
        \item $V_{ik}=\sum_{j \in K_k\setminus K_i} X_j$ is a sum dependent on $X_k$ but not $X_i$.
    \end{itemize}
\end{definition}

From now on, we will assume the random variable $W = \sum_i X_i$ admits a dissociated decomposition as described above.

\begin{theorem}\label{thm:stein_equation_normal_dependent}
     Let $X_i$ be random variables indexed by $i\in I$, s.t. 
     \begin{align*}
        \mathbb{E}[X_i]&=0\\
        \mathrm{Var}(W) &= 1 
     \end{align*}
     where $W = \sum_{i\in I} X_i$ admits a dissociated decomposition and satisfies:
     \begin{equation}\label{eq:equivalence_class_of_decomposition}
        k \in K_i \iff i \in K_k \quad \text{for all $i,k\in I$}     
     \end{equation}
     Then for any continuous and piecewise continuously differentiable $h$ with $\mathbb{E}[|h(X)|]<\infty$:

    \begin{equation*}
        \left|\mathbb{E}[h(W)] - \mathbb{E}[h(X)]\right| \leq 4||h^{\prime}||_\infty \sum_{i\in I}\sum_{j,k\in K_{i}}\left(\mathbb{E}(|X_{i}X_{j}X_{k}|+\mathbb{E}(|X_{i}X_{j}|)\mathbb{E}(|X_{k}|)\right)
    \end{equation*}
    where the $X$ without subscript is $X \sim \mathcal{N}(0,1)$.  
\end{theorem}

\begin{remark}
    Here we have relaxed the condition to allow individual $X_i$ to have different variances, but the total variance is still 1.
\end{remark}

\begin{proof}
    Recall $W = W_i + Z_i$ and $Z_i = \sum_{k\in K_i} X_k$. Then we can write:
    \begin{align*}
        1 &= \mathbb{E}[W^2] = \sum_{i\in I} \mathbb{E}[X_i W] \\
        &= \sum_{i\in I} \mathbb{E}[X_i W] - \mathbb{E}[X_i W_i] \quad \text{($W_i$ is independent of $X_i$)}
         \\
        &= \sum_{i\in I} \mathbb{E}[X_i Z_i] \\
        &= \sum_{i\in I}\sum_{k\in K_i} \mathbb{E}[X_i X_k]
    \end{align*}
    Now since the goal is to bound $|\mathbb{E}[Wg(W)-g'(W)]|$, we can use the same technique as before:
    \begin{align*}
        \mathbb{E}[Wg(W)-g'(W)] &= \underbrace{\left\{\mathbb{E}[W g(W)]-\sum_{i\in I}\textcolor{red}{\mathbb{E}[X_{i}Z_{i}g^{\prime}(W_{i})]}\right\}}_{R_1} \\
        &+\underbrace{\sum_{i\in I}\left\{\textcolor{red}{{\mathbb{E}}[X_{i}Z_{i}g^{\prime}(W_{i})]}-\sum_{k\in K_{i}}\textcolor{blue}{{\mathbb{E}}[X_{i}X_{k}]{\mathbb{E}}[g^{\prime}(W_{i k})]}\right\}}_{R_2}\\
        &+\underbrace{\sum_{i\in I}\sum_{k\in K_{i}}\textcolor{blue}{\mathbb{E}[X_{i}X_{k}]\operatorname{\mathbb{E}}[g^{\prime}(W_{i k})]}-\mathbb{E}[X_{i}X_{k}]\operatorname{\mathbb{E}}[g^{\prime}(W)]}_{R_3}
    \end{align*}
    where in the last sum we used $1=\sum_{i\in I}\sum_{k\in K_i} \mathbb{E}[X_i X_k]$ as computed above and the addition and subtraction of the colored terms; the choice of those terms is motivated by the proof below, where we use Taylor expansion.

\paragraph{Bounding $R_1$} For $R_1$, we have by Taylor expansion at $W_i$:
\begin{align*}
    Wg(W) &= \sum_{i\in I} X_i g(W)\\
    &= \sum_{i\in I} X_i g(W_i + Z_i)\\
    &= \sum_{i\in I} X_i g(W_i) + \textcolor{red}{X_i Z_i g'(W_i)} + \frac{X_i Z_i^2}{2} g''(W_i + \theta_i Z_i)
\end{align*}
for some $\theta_i \in (0,1)$. Taking expectation and using \Cref{def:dissociated_decomposition} make the first term zero, thus the only first-order term is $\textcolor{red}{\mathbb{E}[X_i Z_i g'(W_i)]}$. Now we have:
\begin{align*}
    \left|\underbrace{\mathbb{E}[W g(W)]-\sum_{i\in I}\textcolor{red}{\mathbb{E}[X_{i}Z_{i}g^{\prime}(W_{i})]}}_{R_1} \right| &= \left|\sum_{i\in I}\mathbb{E}\left[\frac{X_{i}Z_{i}^{2}}{2}g^{\prime \prime}\left(W_{i}+\theta_{i} Z_{i}\right)\right]\right|\\
    &\leq \left|\sum_{i\in I}\mathbb{E}\left[\frac{X_{i}Z_{i}^{2}}{2}\|g^{\prime \prime}\|_{\infty}\right]\right|\\
    &\leq\frac{1}{2} \|g^{\prime \prime}\|_{\infty} \sum_{i\in I}\mathbb{E}\left[|X_{i}| Z_{i}^{2}\right]
\end{align*}  

Using $Z_i = \sum_{k\in K_i} X_k$, we can re-write this as:
\begin{align*}
    \frac{1}{2} \|g^{\prime \prime}\|_{\infty} \sum_{i\in I}\sum_{j,k\in K_{i}}\mathbb{E}[|X_{i}|X_{j}X_{k}]
\end{align*}


Thus, we have:
\begin{equation}\label{eq:bound_R1}
    |R_1| \leq \frac{1}{2} \|g^{\prime \prime}\|_{\infty} \sum_{i\in I}\sum_{j,k\in K_{i}}\mathbb{E}[|X_{i}X_{j}X_{k}|]
\end{equation}  


\paragraph{Bounding $R_2$} For $R_2$, recall that $W_{ik}$ is independent of $X_i$ and $X_k$, so we have:

\begin{align*}
    \underbrace{\sum_{i\in I}\left\{\textcolor{red}{{\mathbb{E}}[X_{i}Z_{i}g^{\prime}(W_{i})]}-\sum_{k\in K_{i}}\textcolor{blue}{{\mathbb{E}}[X_{i}X_{k}]{\mathbb{E}}[g^{\prime}(W_{i k})]}\right\}}_{R_2} &= \sum_{i\in I}\left\{\textcolor{red}{{\mathbb{E}}[X_{i}Z_{i}g^{\prime}(W_{i})]}-\sum_{k\in K_{i}}{\textcolor{blue}{\mathbb{E}[X_{i}X_{k}g^{\prime}(W_{i k})]}}\right\}
\end{align*}
Using Taylor expansion again at $W_{ik}$ and recall $W_i= W_{ik} + V_{ik}$,  we have:
\begin{align*}
    \textcolor{red}{X_i Z_i g'(W_i)} &= X_i g'(W_i) \sum_{k\in K_i} X_k\\
    &= X_i (g'(W_{ik}) + V_{ik} g''(W_{ik} + \theta_{ik} V_{ik}) )\sum_{k\in K_i} X_k  \\
    &= \sum_{k\in K_i} {\textcolor{blue}{X_{i}X_{k}g^{\prime}(W_{i k})}} + X_i X_k V_{ik} g''(W_{ik} + \theta_{ik} V_{ik}) 
\end{align*}

for some $\theta_{ik} \in (0,1)$. So again we will cancel the term ${\textcolor{blue}{X_{i}X_{k}g^{\prime}(W_{i k})}}$ in Taylor expansion.

Taking expectation and subtraction yields:
\begin{equation*}
    \sum_{i\in I}\left\{\textcolor{red}{{\mathbb{E}}[X_{i}Z_{i}g^{\prime}(W_{i})]}-\sum_{k\in K_{i}}{\textcolor{blue}{\mathbb{E}[X_{i}X_{k}g^{\prime}(W_{i k})]}}\right\} = \sum_{i\in I}\sum_{k\in K_{i}}{\mathbb{E}}[X_{i}X_{k} V_{ik} g^{\prime \prime}(W_{i k}+\theta_{i k} V_{i k})]
\end{equation*}

Using the definition of $V_{ik} = W_i - W_{ik}=\sum_{j \in K_k\setminus K_i} X_j$ and triangle inequality:
\begin{align*}
    \left|\sum_{i\in I}\sum_{k\in K_{i}}{\mathbb{E}}[X_{i}X_{k} V_{ik} g^{\prime \prime}(W_{i k}+\theta_{i k} V_{i k})]\right| &\leq \sum_{i\in I}\sum_{k\in K_{i}}{\mathbb{E}}[|X_{i}X_{k}| |V_{ik}| \|g^{\prime \prime}\|_{\infty}]\\
    &\leq \|g^{\prime \prime}\|_{\infty} \sum_{i\in I}\sum_{k\in K_{i}}\sum_{j\in K_k\setminus K_i}{\mathbb{E}}[|X_{i}X_{k}X_{j}|]\\
    &= \|g^{\prime \prime}\|_{\infty} \sum_{k\in I}\sum_{i\in K_{k}}\sum_{j\in K_k\setminus K_i}{\mathbb{E}}[|X_{i}X_{k}X_{j}|]\\
    &\leq \|g^{\prime \prime}\|_{\infty} \sum_{k\in I}\sum_{i\in K_{k}}\sum_{j\in K_k}{\mathbb{E}}[|X_{i}X_{k}X_{j}|]
\end{align*}

where in the penultimate step we swapped the indices $i$ and $k$ using the fact that $k \in K_i \iff i \in K_k$, so that $K_k\setminus K_i = K_i\setminus K_k$; the last step follows from the non-negativity of the absolute value.  

(Note that if we do not swap the indices, then the second sum is restricted to $K_i$). Using symmetry, we can re-write the final bound as:  

\begin{equation}\label{eq:R2_bound}
    |R_2| \leq \|g^{\prime \prime}\|_{\infty} \sum_{i\in I}\sum_{j, k\in K_{i}}{\mathbb{E}}[|X_{i}X_{k}X_{j}|]
\end{equation}

\paragraph{Bounding $R_3$} For $R_3$, we expand $g'(W_{ik})$ using Taylor expansion at $W$ for some $\rho_{ik} \in (0,1)$:

\begin{align*}
    \left\|\operatorname{\mathbb{E}}[g^{\prime}(W_{i k})]-\operatorname{\mathbb{E}}[g^{\prime}(W)]\right\| &=\left|\operatorname{\mathbb{E}}[g^{\prime}(W) - (Z_i+V_{ik}) g''(W_{ik} - \rho_{ik} (Z_i+V_{ik}))] - \operatorname{\mathbb{E}}[g^{\prime}(W)]\right|\\
    &\leq \|g''\|_\infty \mathbb{E}[|Z_i+V_{ik}|]
\end{align*}

(recall that $W = W_{ik} + V_{ik} + Z_i$). Now $R_3$ can be bounded as follows:
\begin{align*}
    \underbrace{\sum_{i\in I}\sum_{k\in K_{i}}\textcolor{blue}{\mathbb{E}[X_{i}X_{k}]\operatorname{\mathbb{E}}[g^{\prime}(W_{i k})]}-\mathbb{E}[X_{i}X_{k}]\operatorname{\mathbb{E}}[g^{\prime}(W)]}_{R_3} &\leq \|g''\|_\infty \sum_{i\in I}\sum_{k\in K_{i}}\mathbb{E}[|X_{i}X_{k}|]\mathbb{E}[|Z_i+V_{ik}|]\\
    &=\|g''\|_\infty \sum_{i\in I}\sum_{k\in K_{i}}\mathbb{E}[|X_{i}X_{k}|]\mathbb{E}[|\sum_{j\in K_i} X_j + \sum_{j\in K_k\setminus K_i} X_j|]\\
    &\leq \|g''\|_\infty \sum_{i\in I}\sum_{k,j\in K_{i}}\mathbb{E}[|X_{i}X_{k}|]\mathbb{E}[|X_j|]\\
\end{align*}

Therefore, 
\begin{equation}\label{eq:R3_bound}
    |R_3| \leq \|g''\|_\infty \sum_{i\in I}\sum_{j,k\in K_{i}}\mathbb{E}[|X_{i}X_{k}|]\mathbb{E}[|X_j|]
\end{equation}

Combining \Cref{eq:bound_R1,eq:R2_bound,eq:R3_bound} and using \Cref{lem:stein_equation_normal_bound} gives the desired result.  

\end{proof}

\begin{remark}
    To understand and memorize what needs to be added and subtracted, we can think about the Taylor expansions:
    \begin{itemize}
        \item For $R_1$, we expand the $g$ in $Wg(W)$ around $W_i$: 
        \[Wg(W) = \sum_i X_i g(W_i) + \textcolor{red}{X_i Z_i g'(W_i)} + \ldots \]
        \item Using Taylor again for $g(W_i)$ at $W_{ik}$, we have:
        \[
        \textcolor{red}{X_i Z_i g'(W_i)} = \sum_{k\in K_i} \textcolor{blue}{X_i X_k g'(W_{ik})} + \ldots
            \]
    \end{itemize}
    So suffice to add and subtract those coloured terms.  
\end{remark}

\subsubsection{Triangles in $\mathcal{G}(n,p)$}
For the number of triangles in an Erd\H{o}s-R\'{e}nyi graph $G=\mathcal{G}(n,p)$, we consider the following setup.  

Let index set be 
\[
\Gamma_n = \{(u,v,w): 1\leq u<v<w\leq n\}
\]

and let 

\[
Y_\alpha = Y_{u,v,w} = \mathbf{1}(u\sim v) \mathbf{1}(v\sim w) \mathbf{1}(w\sim u)
\]

which equals 1 if and only if $uvw$ is a triangle. Also let $T$ be:
\[
T = \sum_{\alpha \in \Gamma_n} Y_\alpha
\]

Again if $|\{u,v,w\}\cap \{a,b,c\}|\leq 1$, then $Y_\alpha$ and $Y_\beta$ are independent. Let $K_\alpha$ contain $\alpha$ and all those tuples $\beta$ such that $|\alpha\cap \beta|\geq 2$:

\begin{equation*}
    K_{\alpha} = \{\beta = (a,b,c) \mid |(u,v,w)\cap (a,b,c)|\geq 2\}
\end{equation*}

Recall from \Cref{thm:stein_equation_normal_dependent} that we need to have centred random variables with variance of sum being $1$. So, we standardise to obtain $X_\alpha$:

\begin{equation}\label{eq:standardise_triangle}    
    X_{\alpha}=\frac{Y_{\alpha}-p^{3}}{\sqrt{\mathrm{Var}(T)}}
\end{equation}


A straightforward calculation shows:
\[
    \sigma^{2}=\operatorname{Var}(T)={\binom{n}{3}}p^{3}[1-p^{3}+3(n-3)p^{2}(1-p)]
\]

to see this, note $\mathrm{Var}(T) = \sum_\alpha \mathrm{Var}(Y_\alpha) + \sum_{\alpha\neq \beta} \mathrm{Cov}(Y_\alpha, Y_\beta)$, where $\mathrm{Var}(Y_\alpha) = p^3(1-p^3)$ and $\mathrm{Cov}(Y_\alpha, Y_\beta) = p^5(1-p)$ if $\alpha$ and $\beta$ share exactly two vertices, and 0 otherwise (for the factors $3(n-3)$, see proof of \Cref{thm:triangle_poisson_dependent}). Now we can write as before $W = \sum_{\alpha \in \Gamma_n} X_\alpha$ with:

\[
    W_{\alpha}=\sum_{\beta\notin K_{\alpha}}X_{\beta}{\mathrm{~and~}}Z_{\alpha}=\sum_{\beta\in K_{\alpha}}X_{\beta}
\]

To obtain a dissociated decomposition of $W$, we further decompose $W_\alpha$ as follows:
\[
    W_{\alpha}=\sum_{\gamma\in K_{\alpha}\cup K_{\beta}}X_{\gamma}+\sum_{\gamma\in K_{\beta}\setminus K_{\alpha}}X_{\gamma}
\]

Applying \Cref{thm:stein_equation_normal_dependent} gives the following result:

\begin{theorem}\label{thm:triangle_gnp_normal}
    In $\mathcal{G}(n,p)$ with $n\geq 3$ and $p\in (0,1)$. Let $W = \sum_{\alpha \in \Gamma_n} X_\alpha$  with $X_\alpha$ as in \Cref{eq:standardise_triangle} be the standardised number of triangles.  
    
    Then for any continuous and piecewise continuously differentiable $h$ with $\mathbb{E}[|h(X)|]<\infty$: 
    \[
        |\mathbb{E} [h(W)]- \mathbb{E}[h(X)]| \leq \frac{4}{3}\times\frac{29n^{5}p^{3}}{\sigma^{3}}||h^{\prime}||
    \]

    If $p$ does not depend on $n$, then this expression is of order $\mathcal{O}(n^{-1})$.
\end{theorem}

\begin{proof}
    Using \Cref{thm:stein_equation_normal_dependent} which gives:
    \begin{equation*}
        \left|\mathbb{E}[h(W)] - \mathbb{E}[h(X)]\right| \leq 4||h^{\prime}||\sum_{\alpha\in I}\sum_{\beta,\gamma\in K_{\alpha}}\left(\mathbb{E}[|X_{\alpha}X_{\beta}X_{\gamma}|]+\mathbb{E}[|X_{\alpha}X_{\beta}|]\mathbb{E}[|X_{\gamma}|]
        \right)
    \end{equation*}
    First, computing the expectations explicitly leads to:
    \begin{equation*}
        \mathbb{E}[|X_{\alpha}|]={\frac{2}{\sigma}}p^{3}(1-p^{3}),\quad\mathbb{E}[|X_{\alpha}|^{2}]={\frac{1}{\sigma^{2}}}p^{3}(1-p^{3})
    \end{equation*}
    and
    \begin{equation*}
        \mathbb{E}[|X_{\alpha}^{3}|]={\frac{1}{\sigma^{3}}}(p^{3}(1-p^{3})^{3}+(1-p^{3})p^{9})\leq2{\frac{p^{3}}{\sigma^{3}}}
    \end{equation*}
    since the numerator is less than $1$. Now for the first double sum, we have:
    \begin{align*}
        \sum_{\alpha\in I}\sum_{\beta,\gamma\in K_{\alpha}}\mathbb{E}[|X_{\alpha}X_{\beta}X_{\gamma}|] 
        &\leq \sum_{\alpha\in I} \mathbb{E}[|X_\alpha|^3] + 3\sum_{\alpha\in I}\sum_{\beta \in K_\alpha, \beta \neq \alpha} \mathbb{E}[|X_\alpha^2 X_\beta|] + \sum_{\alpha \in I} \sum_{\beta, \gamma \in K_\alpha\setminus \{\alpha\}, \beta \neq \gamma} \mathbb{E}[|X_\alpha X_\beta X_\gamma|]\\
        &\leq \sum_{\alpha\in I} \mathbb{E}[|X_\alpha|^3] + 3\sum_{\alpha\in I}\sum_{\beta \in K_\alpha, \beta \neq \alpha} \frac{1}{\sigma}\mathbb{E}[|X_\alpha^2|] + \sum_{\alpha \in I} \sum_{\beta, \gamma \in K_\alpha\setminus \{\alpha\}, \beta \neq \gamma} \left(\frac{1}{\sigma}\right)^2\mathbb{E}[|X_\alpha|]\\
        &\leq \binom{n}{3} \frac{2p^3}{\sigma^3} + 3\binom{n}{3} (3n) \frac{1}{\sigma} \frac{p^3(1-p^3)}{\sigma^2} + \binom{n}{3} (3n)^2 \left(\frac{1}{\sigma}\right)^2 \frac{2}{\sigma}p^{3}(1-p^{3}) \\
        &\leq \binom{n}{3} \frac{22n^2p^3}{\sigma^3} < \frac{11n^5p^3}{3\sigma^3}
    \end{align*}

    where we used the fact that $|K_\alpha| = 3n-3 + 1 \leq 3n$ and $|X_\alpha| \leq \frac{1}{\sigma}$ along with the expectations computed above.   

    In the penultimate step above, we computed $2 + 9n + 18n^2 \leq n^2 + 3n^2 + 18n^2 = 22n^2$.

    For the second double sum, we have:
    \begin{align*}
        \sum_{\alpha\in I}\sum_{\beta,\gamma\in K_{\alpha}}\mathbb{E}[|X_{\alpha}X_{\beta}|]\mathbb{E}[|X_{\gamma}|] &\leq \frac{1}{\sigma}  \sum_{\alpha\in I}\sum_{\beta,\gamma\in K_{\alpha}}\mathbb{E}[|X_{\beta}|] \mathbb{E}[|X_{\gamma}|] \\
        &\leq \frac{1}{\sigma} \binom{n}{3} (3n)^2 \left(\frac{2}{\sigma}p^{3}(1-p^{3})\right)^2 \\
        &=\binom{n}{3} \left(\frac{36n^2}{\sigma^3}p^{3}(1-p^{3})\right) \\
        &< \frac{6n^5}{\sigma^3} p^3
    \end{align*}
    Summing those two gives the desired result.
\end{proof}

\begin{remark}
    To see $|X_\alpha| \leq \frac{1}{\sigma}$, note that $|X_\alpha| = \frac{|Y_\alpha - p^3|}{\sigma} \leq \frac{1}{\sigma}$ as $Y_\alpha \in \{0,1\}$ and $p\in (0,1)$.
\end{remark}

\subsubsection{Dense and Sparse Regimes}  

Note that in \Cref{thm:triangle_gnp_normal}, we require the factor $\frac{n^5}{\sigma^3}$ to be small in order to obtain a useful bound. This is the case when $p$ does not depend on $n$, as $\sigma^2 \sim n^4$. However, if for example $p=1/n$, then $\sigma$ is of order $\mathcal{O}(1)$, so the bound can be meaningless.  Those two cases in fact correspond to the dense and sparse regimes of $\mathcal{G}(n,p)$.  

\begin{definition}
    We say a function $f(n)$ satisfies:
    \[
        f(n) = \Theta(g(n))
        \]

    if there exists $c_1, c_2 > 0$ and $n_0$ such that for all $n\geq n_0$, we have:
    \[
        c_1 g(n) \leq f(n) \leq c_2 g(n)
    \]
\end{definition}

We can now define the dense and sparse regimes for Erd\"os-R\'enyi random graphs.

\begin{definition}
    Let $G=\mathcal{G}(n,p)$. We have the following:
    \begin{itemize}
        \item $G$ is \textbf{sparse} if $p=\Theta(c/n)$ for some $c=\Theta(1)$ or equivalently $\binom{n}{2}p=\Theta(1)$.
        \item $G$ is \textbf{dense} if $p=\Theta(1)$ or equivalently $\binom{n}{2}p=\Theta(n^2)$.
        \item $G$ is \textbf{moderately dense} if $p=\Theta(c\log n/n)$ for some $c=\Theta(1)$ or equivalently $\binom{n}{2}p=\Theta(n\log n)$.
    \end{itemize}
\end{definition}

Therefore, as discussed above, the Poisson approximation is more useful in the sparse regime, while the normal approximation is more useful in the dense regime.  


\newpage
\bibliographystyle{apalike}
\bibliography{bibliography3.bib}


\end{document}