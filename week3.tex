\documentclass{article}

\input{includes/commands.tex}
\input{includes/theoremstyle.tex}
\usepackage{tikz}
\usepackage[colon, sort&compress]{natbib}
\title{Week 3 \& 4 Stein's Method for Graphs}

\date{\today}

\begin{document}
% \author{\aut}
\maketitle

\section{Stein's Method for Graphs}

\subsection{Motivation}
Consider the problem of counting the number of triangles in the Erd\H{o}s-R\'{e}nyi random graph $\mathcal{G}(n,p)$. We know that the expected number of triangles is $\binom{n}{3}p^3$. However, we would like to know the (approximate) distribution of the number of triangles. Central limit theorem is not applicable here, since the number of triangles is not a sum of independent random variables: 
\begin{equation*}
    T=\sum_{i<j<k} A_{ij}A_{jk}A_{ki}
\end{equation*}

the random variable $X_{i,j,k}=A_{ij}A_{jk}A_{ki}$ is not independent of one another as they may share edges.  

Stein's Method allows us to approximate the distribution of $T$ by another known distribution and we also get an idea of convergence rate.  

We introduce the general idea of Stein's Method and then consider the cases of a Poisson approximation and a normal approximation.

\subsection{Basic Ingredients}

\begin{unexaminable}
    This section is not examinable. The content of this section is based on \citep{anastasiou2022steins}.
\end{unexaminable}

The main idea works as follows: if we have a random variable $Z$ following an unknown distribution $Q$, we can show it satisfies some equation $\mathbb{E}[(\mathcal{T}f (Z))]=0, \forall f$ if and only if it follows distribution $P$. In particular, it is approximately $P$-distributed if we can show that $\mathbb{E}[(\mathcal{T}f (Z))]\approx 0$ for all $f$.  

More formally, we set up Stein's method for a target probability measure $P$ and any other measure $Q$. Let $\mathcal{G}(\mathcal{T})$ be a set of functions determined by a linear operator $\mathcal{T}$.  

\begin{definition}\label{def:stein_operator}
    The \textbf{Stein operator} $\mathcal{T}$ is a linear operator such that
    \begin{center}
        $P=Q$ if and only if $\mathbb{E}_{Z\sim Q}[(\mathcal{T}g(Z))]=0$ for all $g \in \mathcal{G}(\mathcal{T})$.
    \end{center}
    The set of functions $\mathcal{G}(\mathcal{T})$ such that $\mathbb{E}_{Z \sim P}[(\mathcal{T}g(Z))]=0, \forall g \in \mathcal{G}(\mathcal{T})$ is called the \textbf{Stein class} of $\mathcal{T}$.
\end{definition}

The equation $\mathbb{E}_{Z\sim Q}[(\mathcal{T}g(Z))]=0$ is called the \textbf{Stein identity}.  

With this set up, we can define the how close is $Z$ a $P$-distributed by introducing a measure for distance using the characterization above:

\begin{definition}
    The \textbf{Stein discrepancy} between $P$ and $Q$ is defined as\label{def:stein_discrepancy}
    \begin{equation*}
        \mathcal{D}(Q, \mathcal{T}, \mathcal{G})=\sup_{g \in \mathcal{G}(\mathcal{T})} \|\mathbb{E}_{Z\sim Q}[(\mathcal{T}g(Z))]\|^*
    \end{equation*}
    for some norm $\|\cdot\|^*$.
\end{definition}

The closer this discrepancy is to zero, the closer $Q$ is to $P$. 

Usually, this discrepancy is determined by various types of integral probability metrics (IPMs).

\begin{definition}\label{def:ipm}
    An \textbf{Integral Probability Metric (IPM)} is a function $d_{\mathcal{H}}$, s.t. 
    \begin{equation*}
        d_\mathcal{H}(P, Q) = \sup_{h \in \mathcal{H}} |\mathbb{E}_{X\sim P}[h(X)] - \mathbb{E}_{Z\sim Q}[h(Z)]|
    \end{equation*}
    The set $\mathcal{H} \subset L^1(P) \cap L^1(Q)$ is the class of test functions; if such function $d_\mathcal{H}$ is a metric, then $\mathcal{H}$ is called \textbf{measure determining}.
\end{definition}

An important example is the total variation distance, which we will use in the next section.  

\begin{example}
    The \textbf{total variation distance} is an IPM with $\mathcal{H}=\{1_A: A \in \mathcal{F}\}$, where $\mathcal{F}$ is the $\sigma$-algebra of events, which admits test functions $h(x) = I(x\in A)$. The total variation distance is defined as  
    \begin{equation}
        d_{TV}(P, Q) = \sup_{A \in \mathcal{F}} |P(A) - Q(A)|
        \label{eq:total_variation_distance}
    \end{equation}
    It also has an alternative representation with $\mathcal{H} = \{f: \|f\|_\infty \leq 1\}$:
    \begin{equation*}
        d_{TV}(P, Q) = \frac{1}{2} \sup_{f \in \mathcal{H}}\left|\int f dP - \int f dQ\right|
    \end{equation*}
    A similar representation when the distributions admit Radon-Nikodym derivatives $\frac{dP}{d\mu}=p$ and $\frac{dQ}{d\mu}=q$ with respect to a $\sigma$-finite measure $\mu$:
    \begin{equation*}
        d_{TV}(P, Q) = \frac{1}{2} \int |p - q| d\mu
    \end{equation*}
\end{example}

The proof for the second representation can be found \href{https://math.stackexchange.com/questions/3287889/show-that-the-total-variation-distance-of-probability-measures-mu-nu-is-equa}{here} and the third can be found \href{https://math.stackexchange.com/questions/1481101/definition-of-the-total-variation-distance-vp-q-frac12-int-p-qd-n}{here}.  

In fact, the concept of Stein discrepancy and general IPMs are related by Stein's equation.

\begin{definition}\label{def:stein_equation}
    The \textbf{Stein's equation} for $h\in \mathcal{H}$ is a functional equation:
    \begin{equation*}
        \mathcal{T}g (z) = h(z) - \mathbb{E}_{X\sim P}[h(X)]
    \end{equation*}
    evaluated over $z$ on the support of $P$, where $g$ is a solution to the Stein equation.
\end{definition}

If the solution $g$ exists, then we can take the expectation over $Q$ to get:
\begin{align*}
    \mathbb{E}_{Z\sim Q}[(\mathcal{T}g(Z))] &= \mathbb{E}_{Z\sim Q}[h(Z) - \mathbb{E}_{X\sim P}[h(X)]]\\
    &= \mathbb{E}_{Z\sim Q}[h(Z)] - \mathbb{E}_{X\sim P}[h(X)]\\
\end{align*}

Now taking the supremum over $h \in \mathcal{H}$, we recover the form of an IPM. 



\subsection{Stein-Chen Method for Poisson Approximation}  
Following the framework in the previous section, we can define the Stein operator (\Cref{def:stein_operator}) for Poisson approximation:
\begin{equation}
    \mathcal{T}g(z) = \lambda g(z) - zg(z)
\end{equation}

for $g: \mathbb{N}_0 \to \mathbb{R}$. This gives a characterization of the Poisson distribution with parameter $\lambda>0$:

\begin{proposition}
    \label{prop: direction1 poisson}
    If $Z\sim \text{Po}(\lambda)$, then $\mathbb{E}[(\mathcal{T}g(Z))]=0$ for all $g$ bounded.
\end{proposition}

\begin{proof}
This is a direct computation, noting the $Z=0$ gives zero expectation when $g$ is bounded:  
    \begin{align*}
        \lambda \mathbb{E}[g(Z+1)] &= \lambda \sum_{k=0}^\infty g(k+1) \frac{\lambda^k}{k!} e^{-\lambda}\\
        &= \lambda \sum_{k=1}^\infty g(k) \frac{\lambda^{k-1}}{(k-1)!} e^{-\lambda}\\
        &= \sum_{k=1}^\infty g(k) \frac{\lambda^k}{(k-1)!} e^{-\lambda}\\
        &= \sum_{k=1}^\infty g(k) \frac{\lambda^k}{k!} e^{-\lambda} \cdot k\\
        &= \mathbb{E}[Zg(Z)]\\
    \end{align*}
\end{proof}


The IPM we will use here is the total variation distance (\Cref{eq:total_variation_distance}), which leads to the Stein's equation (\Cref{def:stein_equation}):

\begin{equation}
    \mathcal{T}g(z) = I(z \in A) - \mathbb{E}[I(X \in A)]
    \label{eq:poisson_stein_equation}
\end{equation}

We show this equation has a \textit{unique} solution.

\begin{lemma}\label{lem:poisson_stein_equation}
    Given the Stein's equation above, we have a unique solution
    \begin{equation}
        g(z+1) = \frac{z!}{\lambda^{z+1}} e^\lambda \sum_{k=0}^z \frac{\lambda^k}{k!} (I(k\in A) - \mathbb{E}[I(k\in A)])
    \end{equation}

    this solution can also be written as 
    \begin{equation}
        g(z+1) = -\frac{z!}{\lambda^{z+1}} e^\lambda \sum_{k=z+1}^\infty \frac{\lambda^k}{k!} (I(k\in A) - \mathbb{E}[I(k\in A)])
    \end{equation}
\end{lemma}

\begin{proof}
    We first show the claimed solution satisfies the Stein's equation, which is easily verified by direct computation. To show uniqueness, let $z=0$ in \Cref{eq:poisson_stein_equation} and we get $\lambda g(1) = I(0\in A) - \mathbb{E}[I(0\in A)]$. So if $f$ is another solution to \Cref{eq:poisson_stein_equation}, then $f(1)=g(1)$. Now we can use induction to show $f(z)=g(z)$ for all $z\in \mathbb{N}_0$.
\end{proof}

To bound the total variation distance, we require additional bounds on $g$.  

\begin{lemma}\label{lem:poisson_stein_bound}
For the solution $g$ to the Stein's equation \Cref{eq:poisson_stein_equation}, we have:
\begin{equation}
    \sup_{k \in \mathbb{N}_0} |g(k)| \leq \min (1, \lambda^{-1/2})
    \label{eq:poisson_stein_bound1}
\end{equation}
and 
\begin{equation}
    \sup_{k \in \mathbb{N}_0} |g(k+1) - g(k)| \leq \min (1, \lambda^{-1})
    \label{eq:poisson_stein_bound2}
\end{equation}

Here $\lambda^{-1}$ is referred to as the \textit{magic factor}.
\end{lemma}

\begin{proof}
    The proof is omitted.
\end{proof}

Now we give the converse of \Cref{prop: direction1 poisson}.

\begin{proposition}\label{prop: direction2 poisson}
    If $\mathbb{E}[(\mathcal{T}g(Z))]=0$ for all $g$ bounded, then $Z\sim \text{Po}(\lambda)$.
\end{proposition}

\begin{proof}
    Since $g$ is an arbitrary bounded function, we take it as the solution obtained in \Cref{lem:poisson_stein_equation}, which is bounded by the previous lemma. Then taking the expectation of \Cref{eq:poisson_stein_equation} finishes the proof.
\end{proof}

Now we can bound the total variation distance between $Z$ and $\text{Po}(\lambda)$:
\begin{equation*}
    d_{TV}(\mathcal{L}(Z), \text{Po}(\lambda)) = \sup_{g} |\mathbb{E}[\lambda g(Z+1) - Zg(Z)]|
\end{equation*}

To see this is actually useful, we consider a bound for Bernoulli random variables. 

\begin{example}\label{ex:bernoulli_poisson}
    Let $X_1, \ldots, X_n$ be independent Bernoulli random variables, where $X_i \sim \text{Ber}(p_i)$ and $Z = \sum_{i=1}^n X_i$. Set $\lambda = \sum_{i=1}^n p_i$. Then we can compute the Stein's equation $\mathbb{E}[\lambda g(Z+1) - Zg(Z)]$.  Now since $X_i$ is binary and $Z-X_i$ is independent of $X_i$, we have:  
    \begin{align*}
        \mathbb{E}[Zg(Z)] &= \mathbb{E}[\sum_{i=1}^n X_i g(Z)]\\
        &= \sum_{i=1}^n \mathbb{E}[X_i g(Z + X_i - 1)]\\
        &= \sum_{i=1}^n \mathbb{E}[X_i] \mathbb{E}[g(Z + X_i - 1)]\\
        &= \sum_{i=1}^n p_i \mathbb{E}[g(Z + X_i - 1)]\\
    \end{align*}

    where in the second step we use the fact that $X_i=0$ when $g$ is bounded yields zero expectation. Thus, 
    \begin{align*}
        \mathbb{E}[\lambda g(Z+1) - Zg(Z)] &= \sum_{i=1}^n p_i \mathbb{E}[g(Z + 1)] - \sum_{i=1}^n p_i \mathbb{E}[g(Z + X_i - 1)]\\
        &= \sum_{i=1}^n p_i \mathbb{E}[\mathbb{E}[g(Z+1) - g(Z + X_i - 1) \mid X_i]]\\
        &= \sum_{i=1}^n p_i^2 \mathbb{E}[g(Z+1) - g(Z) \mid X_i=1]\\
        & \leq \sum_{i=1}^n p_i^2 \mathbb{E}[|g(Z+1) - g(Z)| \mid X_i=1]
    \end{align*}
    where we used conditional expectation.   
    Using using \Cref{lem:poisson_stein_bound} now gives 
    \[|\mathbb{E}[\lambda g(Z+1) - Zg(Z)]| \leq \min (1, \lambda^{-1}) \sum_{i=1}^n p_i^2\]
    So the total variation distance is bounded by
    \begin{equation*}
        d_{TV}(\mathcal{L}(Z), \text{Po}(\lambda)) \leq \min (1, \lambda^{-1}) \sum_{i=1}^n p_i^2
    \end{equation*}
    Note this bound is non-asympotic and holds for any $n$.
\end{example}

\subsubsection{Local dependence}  
When the random variables are not independent, the computation of $\mathbb{E}[Wg(W)]$ is more complicated.  Here we let $A_\alpha$ denote the set elements that are dependent on $X_\alpha$.  

\begin{theorem}\label{thm:poisson_stein_local}
    Let $X_\alpha, \alpha \in I$ with each $X_\alpha \sim \text{Ber}(p_\alpha)$ and $Z = \sum_{\alpha \in I} X_\alpha$. Let $\lambda = \sum_{\alpha \in I} p_\alpha$. Suppose $\forall \alpha \in I$, there exists a set $A_\alpha \subseteq I$, s.t. $X_\alpha$ is independent of $\sum_{\beta \notin A_\alpha} X_\beta$. Define 
    \begin{equation*}
        \eta_\alpha = \sum_{\beta \in A_\alpha} X_\beta
    \end{equation*}
    Then the total variation distance is bounded by
    \begin{equation*}
        d_{TV}(\mathcal{L}(Z), \text{Po}(\lambda)) \leq \sum_{\alpha\in I}[(p_{\alpha}\mathbb{E}(\eta_{\alpha})+\mathbb{E}(X_{\alpha}(\eta_{\alpha}-X_{\alpha}))]\operatorname*{min}\left(1,\lambda^{-1}\right)
    \end{equation*}
\end{theorem}

\begin{proof}

    As before, we aim to bound the total variation distance by bounding $\mathbb{E}|\lambda g(Z+1) - Zg(Z)|$, where $g$ is the solution to the Stein's equation.  

    Let $Z_\alpha = Z - \eta_\alpha$, we compute the term $\mathbb{E}[Zg(Z)]$ as follows:  
    \begin{align*}
        \mathbb{E}[Zg(Z)] &= \mathbb{E}[\sum_{\alpha \in I} X_\alpha g(Z)]\\
        &= \sum_{\alpha \in I} \mathbb{E}[X_\alpha g(Z - X_\alpha + 1)]\\
        &= \sum_{\alpha \in I} \mathbb{E}[X_\alpha g(Z_\alpha + \eta_\alpha -X_\alpha +  1)]\\
        &= \sum_{\alpha \in I} \mathbb{E}[X_\alpha g(Z_\alpha + 1)] + \mathbb{E}[X_\alpha (g(Z_\alpha + \eta_\alpha -X_\alpha +  1) - g(Z_\alpha + 1))]\\
        &= \sum_{\alpha \in I} p_\alpha \mathbb{E}[g(Z_\alpha + 1)] \\
        &+ p_\alpha \mathbb{E}[g(Z+1)] - p_\alpha \mathbb{E}[g(Z+1)] \\
        &+ \mathbb{E}[X_\alpha (g(Z_\alpha + \eta_\alpha -X_\alpha +  1) - g(Z_\alpha + 1))] \\
    \end{align*}

    Rearranging the last term leads to 
    \[\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(Z + 1)] + \underbrace{p_\alpha \mathbb{E}[g(Z_\alpha+1) - g(Z+1)]}_{R_{1,\alpha}} + \underbrace{\mathbb{E}[X_\alpha (g(Z_\alpha + \eta_\alpha -X_\alpha +  1) - g(Z_\alpha + 1))]}_{R_{2, \alpha}}\] 

    Hence the equation we need to bound is
    \begin{equation*}
        \mathbb{E}[\lambda g(Z+1) - Zg(Z)] =  R_1 + R_2
    \end{equation*}

    For $R_1=\sum_\alpha R_{1,\alpha}$, we have
    \begin{align*}
        |R_1| &= \left|\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(Z_\alpha+1) - g(Z+1)]\right| \\
        &= \left|\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(Z_\alpha + \eta_\alpha +1) - g(Z_\alpha+1)]\right|\\
        &\leq \sum_{\alpha \in I} p_\alpha |\mathbb{E}[g(Z_\alpha + \eta_\alpha +1) - g(Z_\alpha+1)]|\\
    \end{align*}
    Now we apply conditional expectation and telescoping sum to the inner term:
    \begin{align*}
        |\mathbb{E}[g(Z_\alpha + \eta_\alpha +1) - g(Z_\alpha+1)]| &= \left|\mathbb{E}\left[\mathbb{E}[\sum_{k=0}^{m-1} g(Z_\alpha + 1 + k+1) - g(Z_\alpha + 1 + k)\mid \eta_\alpha = m]\right]\right|\\
        &\leq \mathbb{E}\left[\mathbb{E}[\sum_{k=0}^{m-1} |g(Z_\alpha + 1 + k+1) - g(Z_\alpha + 1 + k)|\mid \eta_\alpha = m]\right]\\
        &\leq \mathbb{E}\left[\mathbb{E}[\sum_{k=0}^{m-1} \min (1, \lambda^{-1})\mid \eta_\alpha = m]\right]\\
        &= \min (1, \lambda^{-1}) \mathbb{E}[ \eta_\alpha]\\
    \end{align*}

    which gives the bound for $R_1$ as:
    \begin{equation*}
        |R_1| \leq \min (1, \lambda^{-1}) \sum_{\alpha \in I} p_\alpha \mathbb{E}[ \eta_\alpha]
    \end{equation*}

    For $R_2=\sum_\alpha R_{2,\alpha}$, we apply the method above to $g(Z_\alpha + \eta_\alpha - X_\alpha + 1) - g(Z_\alpha + 1)$, which gives the bound:
    \begin{equation*}
        |R_2| \leq \min (1, \lambda^{-1}) \sum_{\alpha \in I} p_\alpha \mathbb{E}[ X_\alpha (\eta_\alpha - X_\alpha)]
    \end{equation*}

    Combing those two bounds gives the desired result.
\end{proof}  

We are now ready to approximate the number of triangles in Erd\H{o}s-R\'{e}nyi random graph $\mathcal{G}(n,p)$.  We use an index set $\Gamma_n$:

\begin{equation*}
    \Gamma_{n}=\{\alpha=(u,v,w):1\leq u<v<w\leq n\}
\end{equation*}

Then with the same notation as before, let 
$$
X_{\alpha}=X_{u,v,w} = \mathbf{1}(u\sim v) \mathbf{1}(v\sim w) \mathbf{1}(w\sim u)
$$

which equals 1 if and only if $uvw$ is a triangle, each of the $X_\alpha$ is a Bernoulli random variable with parameter $p^3$. 

\begin{theorem}\label{thm:triangle_poisson_dependent}
    Let $Z = \sum_{\alpha \in \Gamma_n} X_\alpha$ and $\lambda = \binom{n}{3}p^3$.
    \begin{equation*}
        d_{TV}(\mathcal{L}(Z), \text{Po}(\lambda)) \leq \binom{n}{3}(3np^3 + 3np^2) \min (1, \lambda^{-1})
    \end{equation*}
\end{theorem}

\begin{proof}
    Using the same notation as above with $p_\alpha=p^3$ and $A_\alpha$ with $\alpha=(u,v,w)$ be defined as 
    \begin{equation*}
        A_{\alpha} = \{(a,b,c) \mid |(u,v,w)\cap (a,b,c)|\geq 2\}
    \end{equation*}
    It is easy to see $|A_\alpha| = 3(n-3) + 1 < 3n$, thus $\mathbb{E}[\eta_\alpha]\leq 3np^3$. Moreover, we have  
    \begin{equation*}
        \mathbb{E}[X_\alpha(\eta_\alpha - X_\alpha)] = \mathbb{E}[X_\alpha \sum_{\beta \in A_\alpha, \beta \neq \alpha} X_\beta] = \sum_{\beta \in A_\alpha, \beta \neq \alpha} p^5 \leq 3np^5
    \end{equation*}
    Using \Cref{thm:poisson_stein_local} gives the desired result.
\end{proof}


\subsection{Stein's Method for Normal Approximation}
Similarly, we set up the Stein operator (\Cref{def:stein_operator}) for normal approximation:  

\begin{equation}\label{eq:stein_operator_normal}
    \mathcal{T}g(z) = g'(z) - zg(z)
\end{equation}

for all continuous and piecewise continuously differentiable functions $g: \mathbb{R} \to \mathbb{R}$ with $\mathbb{E}[|g'(Z)|]<\infty$. This gives a characterization of the normal distribution with mean $\mu$ and variance $\sigma^2$:  

\begin{theorem}[\citep{stein1972bound}]\label{thm:stein_operator_normal}
    If $Z\sim \mathcal{N}(0, 1)$, then 
    \begin{equation*}
        \mathbb{E}[(\mathcal{T}g(Z))]:=\mathbb{E}[Zg(Z) - g'(Z)] = 0
    \end{equation*}
\end{theorem}

\begin{proof}
    We can compute $\mathbb{E}[g'(Z)]$ using integration by parts:
    \begin{align*}
        \mathbb{E}[g'(Z)] &= \int_{-\infty}^\infty g'(z) \frac{1}{\sqrt{2\pi}} e^{-z^2/2} dz\\
        &= {\frac{1}{\sqrt{2\pi}}}\int_{-\infty}^{0}g^{\prime}(z)\left(\int_{-\infty}^{z}-x e^{-x^{2}/2}d x\right)\,d z \\
        &+ {\frac{1}{\sqrt{2\pi}}}\int_{0}^{\infty}g^{\prime}(z)\left(\int_{z}^{\infty}x e^{-x^{2}/2}d x\right)\,d z
    \end{align*}

    Now we exchange the order of integration by Fubini's theorem:
    \begin{align*}
        \mathbb{E}[g'(Z)] &= \int_{x=-\infty}^0 \left(\int_{z=x}^0 g'(z) dz\right) (-x) e^{-x^2/2} dx + \int_{x=0}^\infty \left(\int_{z=0}^x g'(z) dz\right) xe^{-x^2/2} dx\\
        &={\frac{1}{\sqrt{2\pi}}}\int_{-\infty}^{\infty}[g(x)-g(0)]x e^{-x^{2}/2}d x \\
        &=\mathbb{E}[Z g(Z)]
    \end{align*}
    as $X$ has mean zero.
    \begin{unexaminable}
        The condition for Fubini is easily checked, as $\int_0^\infty |g'(z)| f_Z(z) dz \leq \mathbb{E}[|g'(z)|] < \infty$.
    \end{unexaminable}
\end{proof}

We do not consider the total variation distance here, but instead use the \textbf{weak convergence} of measures, which is motivated by the following exmaple.  

\begin{example}[(Total vairation might not be the best choice)]

    Let $X\sim \mathrm{Binom}(n,p)$ and $Y\sim \mathcal{N}(0,1)$, then we know from the central limit theorem that
    \begin{equation*}
        \frac{X-np}{\sqrt{np(1-p)}} \xrightarrow{d} Y
    \end{equation*}
    where $\xrightarrow{d}$ denotes weak convergence. However, if we have set $A_n :=\{0, 1, \ldots, n\}$, then $\mathbb{P}(X\in A_n) = 1$ and $\mathbb{P}(Y\in A_n) = 0$, for all $n\in \mathbb{N}$.
\end{example}

We now define the notion of weak convergence of probability measures.  

\begin{definition}
   Let $(P_n)_{n\in \mathbb{N}}$ be a sequence of probability measures on $\mathbb{R}$ and $P$ be another probability measure on $\mathbb{R}$. Then \textbf{weak convergence} is equivalent to the following statements:
   \begin{itemize}
    \item $P_n(A) \to P(A)$ for all $A \in \mathcal{B}(\mathbb{R})$ such that $P(\partial A)=0$.
    \item $\int f dP_n \to \int f dP$ for all bounded continuous functions $f$.
    \item $\int f dP_n \to \int f dP$ for all bounded, infintely-often differentiable continuous functions $f$.
   \end{itemize}
\end{definition}

This motivates the Steins's equation (\Cref{def:stein_equation}):
\begin{equation}\label{eq:stein_equation_normal}
    \mathcal{T}g(z) = h(z) - \mathbb{E}[h(X)]
\end{equation}

where $h$ is a bounded measurable function.


\begin{lemma}\label{lem:stein_equation_normal}
    Given the Stein's equation above, we have a unique bounded solution $g_h$, if:
    \begin{itemize}
        \item $h:\mathbb{R} \to \mathbb{R}$ is bounded and measurable.
        \item $\mathbb{E}[|h(Z)|] < \infty$.
        \item $h$ is continuous and piecewise continuously differentiable with $\|h'\|_\infty < \infty$.
    \end{itemize}

        Then the solution can be written as
        \begin{equation*}
            g_h(z) = e^{z^2/2} \int_{-\infty}^z (h(w) - \mathbb{E}[h(X)]) e^{-w^2/2} dw
        \end{equation*}
        which can also be written as
        \begin{equation*}
            g_h(z) = -e^{z^2/2} \int_{z}^\infty (h(w) - \mathbb{E}[h(X)]) e^{-w^2/2} dw
        \end{equation*}
    \end{lemma}

    \begin{proof}
        Multiplying both sides of \Cref{eq:stein_equation_normal} by $e^{-z^2/2}$, we note:
        \begin{align*}
            e^{-z^2/2} g'(z) - e^{-z^2/2}zg(z) &= e^{-z^2/2} (h(z) - \mathbb{E}[h(X)]) 
        \end{align*}
        The LHS is simply the derivative of $e^{-z^2/2}g(z)$, so we can integrate both sides to get:
        \begin{align*}
            e^{-z^2/2}g(z) &= \int_{-\infty}^z e^{-w^2/2} (h(w) - \mathbb{E}[h(X)]) dw +C\\
            g(z) &= e^{z^2/2} \int_{-\infty}^z e^{-w^2/2} (h(w) - \mathbb{E}[h(X)]) dw + Ce^{z^2/2}\\
        \end{align*}
        Note the solution $g$ is bounded if and only if $C=0$ as otherwise $g\to infty$ as $z\to \infty$.  
        To obtain the second form, we note that 
        \begin{equation*}
            \int_{-\infty}^{+\infty} (h(w) - \mathbb{E}[h(X)]) e^{-w^2/2} dw = 0
        \end{equation*}
        as $e^{-w^2/2}$ is the density of $\mathcal{N}(0,1)$, so the second form follows from symmetry.
    \end{proof}

The bounds on the solution is given by the following lemma.  
\begin{lemma}\label{lem:stein_equation_normal_bound}
    Given $h$ bounded and $g_h$ being its solution in \Cref{lem:stein_equation_normal}, we have:
    \begin{equation*}
        \|g_h\|_\infty \leq \sqrt{\frac{\pi}{2}} \|h - \mathbb{E}[h(X)]\|_\infty
    \end{equation*}
    and 
    \begin{equation*}
        \|g_h'\|_\infty \leq 2 \|h-\mathbb{E}[h(X)]\|_\infty
    \end{equation*}
    For $h$ being continuous and piecewise continuously differentiable, we have:
    \begin{align*}
        \|g_h\|_\infty &\leq 2 \|h'\|_\infty \\
        \|g_h'\|_\infty &\leq \sqrt{\frac{2}{\pi}} \|h'\|_\infty\\
        \|g_h''\|_\infty &\leq 2 \|h'\|_\infty
    \end{align*}
\end{lemma}
\begin{proof}
    The proof is omitted.
\end{proof}

We can now give the converse of \Cref{thm:stein_operator_normal} to characterize the normal distribution.  

\begin{theorem}[\citep{stein1986}]
    If $\mathbb{E}[(\mathcal{T}g(Z))]=\mathbb{E}[Zg(Z) - g'(Z)] = 0$ for all bounded $g$, then $Z\sim \mathcal{N}(0,1)$.
\end{theorem}

\begin{proof}
    The idea is to take a particular $h$ and use weak convergence. Let $h$ be defined as 
    \begin{equation*}
        h_w(z) = \mathbf{1}_{(-\infty, w]}(z)
    \end{equation*}
    where $w\in \mathbb{R}$. Then it is clearly continuous (a.s.) and piecewise continuously differentiable. Moreover, since the solution $g_h$ is in particular bounded by the previous lemma, we have:  
    \begin{equation*}
        0 = \mathbb{E}[Zg_h(Z)-g_h'(Z)] = \mathbb{E}[h(Z)] - \mathbb{E}[h(X)]=\mathbb{P}(Z\leq w) - \Phi(X\leq w)
    \end{equation*}
    which implies $Z\sim \mathcal{N}(0,1)$ as $w$ is arbitrary.
\end{proof}

Now for independent $X_1, \ldots, X_n$ with zero mean and variance $1/n$, we can bound the weak convergence of their sum to the standard normal distribution.  

\begin{theorem}
    Let $Z = \sum_{i=1}^{n} X_i$ with $X_i$ being independent and $\mathbb{E}[X_i]=0$ and $\mathbb{E}[X_i^2]=1/n$. Then we have for any continuous and piecewise continuously differentiable $h$:
    \begin{equation*}
        \left|\mathbb{E}[h(Z)] - \mathbb{E}[h(X)]\right| \leq \|h'\|_\infty \left( \frac{2}{\sqrt{n}} + \sum_{i=1}^{n} \mathbb{E}[|X_i|^3]\right)
    \end{equation*}
    where $X\sim \mathcal{N}(0,1)$.
\end{theorem}

\begin{proof}
    We would like to bound $\mathbb{E}[g'(Z)-Zg(Z)]$. Let $Z_i = Z - X_i$, so $Z_i$ is independent of $X_i$. Then we have:
    \begin{align*}
        \mathbb{E}[Zg(Z)] &= \mathbb{E}[\sum_{i=1}^{n} X_i g(Z)]\\
        &= \sum_{i=1}^{n} \mathbb{E}[X_i g(Z_i + X_i)]\\
        &= \sum_{i=1}^{n} \mathbb{E}[X_i (g(Z_i) + X_i g'(Z_i) + \frac{X_i^2}{2} g''(Z_i +\theta X_i))]\\
        &= \sum_{i=1}^{n} \mathbb{E}[X_i] \mathbb{E}[g(Z_i)] + \mathbb{E}[X_i^2] \mathbb{E}[g'(Z_i)] + \frac{1}{2} \mathbb{E}[X_i^3] \mathbb{E}[g''(Z_i +\theta X_i)]\\
        &= \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[g'(Z_i)] + R
    \end{align*}

    where $R$ is the remainder term. Now we bound $R$ as follows:
    \begin{align*}
        |R| &\leq \frac{1}{2} \sum_{i=1}^{n} \mathbb{E}[|X_i|^3] \|g''\|_\infty\\
        &\leq \sum_{i=1}^{n} \mathbb{E}[|X_i|^3] \|h'\|_\infty\\
    \end{align*}

    Therefore, we have:
    \begin{equation*}
        \mathbb{E}[g'(Z)-Zg(Z)] = \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[g'(Z) -g'(Z_i)] - R
    \end{equation*}

    Again by Taylor expansion or the mean value theorem, we have:
    \begin{align*}
        \left|\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[g'(Z) -g'(Z_i)]\right| &\leq \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[|Z-Z_i| \|g''\|_\infty]\\ 
        &\leq \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[|X_i| \|g''\|_\infty] \\
        &\leq \frac{2}{\sqrt{n}} \|h''\|_\infty
    \end{align*}
    where we have used \Cref{lem:stein_equation_normal_bound} and Cauchy-Schwarz inequality in the last step (namely, $\mathbb{E}[|X_i| \cdot 1]^2 \leq \mathbb{E}[|X_i|^2] \cdot 1=1/n \implies \mathbb{E}[|X_i|]\leq 1/\sqrt{n}$). Combining the two bounds using triangle inequality gives the desired result.
\end{proof}


\subsubsection{Dissociated Decompositions}
We will use the same notation as the lecture notes with $W=\sum_{i\in I} X_i$, where $I$ is a finite index set and $X_i$ are random variables.

\begin{definition}\label{def:dissociated_decomposition}
    A \textbf{dissociated decomposition} of $W$ for index $i$ is 
    \begin{align*}
        W &= W_i + Z_i \\
        W &= W_{ik} + V_{ik} + Z_i
    \end{align*}
    where we assume there exists $K_i\subseteq I$ for each $i\in I$ with the following:
    \begin{itemize}
        \item $W_i$ is independent of $X_i$.
        \item $Z_i=\sum_{k\in K_i} X_k$ is a sum dependent on $X_i$.
        \item $W_{ik}$ is independent of $X_i$ and $X_k$, for $k\in K_i$.
        \item $V_{ik}=\sum_{j \in K_k\setminus K_i} X_j$ is a sum dependent on $X_k$ but not $X_i$.
    \end{itemize}
\end{definition}

We now provide a bound for $W$ in the case of dependent random variables.

\begin{theorem}\label{thm:stein_equation_normal_dependent}
    Assume $W$ admits a dissociated decomposition as decribed above. Let $X_i, i\in I$ with $\mathbb{E}[X_i]=0$ and $\mathrm{Var}(W)=1$. Then for any continuous and piecewise continuously differentiable $h$ with $\mathbb{E}[|h(X)|]<\infty$:

    \begin{equation*}
        \left|\mathbb{E}[h(W)] - \mathbb{E}[h(X)]\right| \leq 4||h^{\prime}||\sum_{i\in I}\sum_{j,k\in K_{i}}\left(\mathbb{E}(|X_{i}X_{j}X_{k}|+\mathbb{E}(|X_{i}X_{j}|)\mathbb{E}(|X_{k}|)\right)
    \end{equation*}
    where the $X$ without subscript is $\mathcal{N}(0,1)$.  
\end{theorem}

\begin{proof}
    Recall $W = W_i + Z_i$ and $Z_i = \sum_{k\in K_i} X_k$ with $W_i$ independent of $X_i$. Then we can write:
    \begin{align*}
        1 &= \mathbb{E}[W^2] = \sum_{i\in I} \mathbb{E}[X_i W] \\
        &= \sum_{i\in I} \mathbb{E}[X_i Z_i] \\
        &= \sum_{i\in I}\sum_{k\in K_i} \mathbb{E}[X_i X_k]
    \end{align*}
    Now since the goal is to bound $|\mathbb{E}[Wg(W)-g'(W)]|$, we can use the same technique as before:
    \begin{align*}
        \mathbb{E}[Wg(W)-g'(W)] &= \underbrace{\left\{\mathbb{E}[W g(W)]-\sum_{i\in I}\mathbb{E}[X_{i}Z_{i}g^{\prime}(W_{i})]\right\}}_{R_1} \\
        &+\underbrace{\sum_{i\in I}\left\{{\mathbb{E}}[X_{i}Z_{i}g^{\prime}(W_{i})]-\sum_{k\in K_{i}}{\mathbb{E}}[X_{i}X_{k}]{\mathbb{E}}[g^{\prime}(W_{i k})]\right\}}_{R_2}\\
        &+\underbrace{\sum_{i\in I}\sum_{k\in K_{i}}\mathbb{E}[X_{i}X_{k}]\left\{\operatorname{\mathbb{E}}[g^{\prime}(W_{i k})]-\operatorname{\mathbb{E}}[g^{\prime}(W)]\right\}}_{R_3}
    \end{align*}
    where in the last sum we used $1=\sum_{i\in I}\sum_{k\in K_i} \mathbb{E}[X_i X_k]$. Now we bound each term separately.


\paragraph{Bounding $R_1$} For $R_1$, we have by Taylor expansion:
\begin{align*}
    Wg(W) &= \sum_{i\in I} X_i g(W_i + Z_i)\\
    &= \sum_{i\in I} X_i g(W_i) + X_i Z_i g'(W_i) + \frac{X_i Z_i^2}{2} g''(W_i + \theta_i Z_i)\\
\end{align*}
for some $\theta_i \in (0,1)$. Taking expectation and using \Cref{def:dissociated_decomposition} make the first term zero, and we have:
\begin{align*}
    \left|\mathbb{E}[W g(W)]-\sum_{i\in I}\mathbb{E}[X_{i}Z_{i}g^{\prime}(W_{i})]\right| &= \left|\sum_{i\in I}\mathbb{E}\left[\frac{X_{i}Z_{i}^{2}}{2}g^{\prime \prime}\left(W_{i}+\theta_{i} Z_{i}\right)\right]\right|\\
    &\leq \left|\sum_{i\in I}\mathbb{E}\left[\frac{X_{i}Z_{i}^{2}}{2}\|g^{\prime \prime}\|_{\infty}\right]\right|\\
    &=\frac{1}{2} \|g^{\prime \prime}\|_{\infty} \sum_{i\in I}\mathbb{E}\left[X_{i} Z_{i}^{2}\right]\\
\end{align*}

which we can re-write as $\frac{1}{2} \|g^{\prime \prime}\|_{\infty} \sum_{i\in I}\sum_{j,k\in K_{i}}\mathbb{E}[|X_{i}|X_{j}X_{k}]$. Thus, we have:
\begin{equation*}
    |R_1| \leq \frac{1}{2} \|g^{\prime \prime}\|_{\infty} \sum_{i\in I}\sum_{j,k\in K_{i}}\mathbb{E}[|X_{i}X_{j}X_{k}|]
\end{equation*}  


\paragraph{Bounding $R_2$} For $R_2$, recall that $W_{ik}$ is independent of $X_i$ and $X_k$, so we have:

\begin{align*}
    \sum_{i\in I}\left\{{\mathbb{E}}[X_{i}Z_{i}g^{\prime}(W_{i})]-\sum_{k\in K_{i}}{\mathbb{E}}[X_{i}X_{k}]{\mathbb{E}}[g^{\prime}(W_{i k})]\right\} &= \sum_{i\in I}\left\{{\mathbb{E}}[X_{i}Z_{i}g^{\prime}(W_{i})]-\sum_{k\in K_{i}}{\mathbb{E}}[X_{i}X_{k} g^{\prime}(W_{i k})]\right\}\\
\end{align*}

Using Taylor expansion again, we have:
\begin{equation*}
    X_i Z_i g'(W_i) = \sum_{k\in K_i} X_i X_k \left( g'(W_{ik}) + V_{ik} g''(W_{ik} + \theta_{ik} V_{ik})\right)
\end{equation*}

for some $\theta_{ik} \in (0,1)$. Taking expectation and subtraction yields:
\begin{equation*}
    \sum_{i\in I}\left\{{\mathbb{E}}[X_{i}Z_{i}g^{\prime}(W_{i})]-\sum_{k\in K_{i}}{\mathbb{E}}[X_{i}X_{k} g^{\prime}(W_{i k})]\right\} = \sum_{i\in I}\sum_{k\in K_{i}}{\mathbb{E}}[X_{i}X_{k} V_{ik} g^{\prime \prime}(W_{i k}+\theta_{i k} V_{i k})]
\end{equation*}

Using the definition of $V_{ik}$ and triangle inequality:
\begin{align*}
    \left|\sum_{i\in I}\sum_{k\in K_{i}}{\mathbb{E}}[X_{i}X_{k} V_{ik} g^{\prime \prime}(W_{i k}+\theta_{i k} V_{i k})]\right| &\leq \sum_{i\in I}\sum_{k\in K_{i}}{\mathbb{E}}[|X_{i}X_{k}| |V_{ik}| \|g^{\prime \prime}\|_{\infty}]\\
    &\leq \|g^{\prime \prime}\|_{\infty} \sum_{i\in I}\sum_{k\in K_{i}}\sum_{j\in K_k\setminus K_i}{\mathbb{E}}[|X_{i}X_{k}X_{j}|]\\
    &= \|g^{\prime \prime}\|_{\infty} \sum_{k\in I}\sum_{i\in K_{k}}\sum_{j\in K_k\setminus K_i}{\mathbb{E}}[|X_{i}X_{k}X_{j}|]\\
    &\leq \|g^{\prime \prime}\|_{\infty} \sum_{k\in I}\sum_{i\in K_{k}}\sum_{j\in K_k}{\mathbb{E}}[|X_{i}X_{k}X_{j}|]\\
\end{align*}

where in the penultimate step we used the fact that $k \in K_i \iff i \in K_k$. 
\paragraph{Bounding $R_3$} For $R_3$, we expand $g'(W_{ik})$ using Taylor expansion for some $\rho_{ik} \in (0,1)$:

\begin{align*}
    \left\|\operatorname{\mathbb{E}}[g^{\prime}(W_{i k})]-\operatorname{\mathbb{E}}[g^{\prime}(W)]\right\| &=\left|\operatorname{\mathbb{E}}[g^{\prime}(W) - (Z_i+V_{ik}) g''(W_{ik} - \rho_{ik} (Z_i+V_{ik}))] - \operatorname{\mathbb{E}}[g^{\prime}(W)]\right|\\
    &\leq \|g''\|_\infty \mathbb{E}[|Z_i+V_{ik}|]\\
\end{align*}

Now this gives:
\begin{align*}
    \sum_{i\in I}\sum_{k\in K_{i}}\mathbb{E}[X_{i}X_{k}]\left\{\operatorname{\mathbb{E}}[g^{\prime}(W_{i k})]-\operatorname{\mathbb{E}}[g^{\prime}(W)]\right\} &\leq \|g''\|_\infty \sum_{i\in I}\sum_{k\in K_{i}}\mathbb{E}[|X_{i}X_{k}|]\mathbb{E}[|Z_i+V_{ik}|]\\
    &=\|g''\|_\infty \sum_{i\in I}\sum_{k\in K_{i}}\mathbb{E}[|X_{i}X_{k}|]\mathbb{E}[|\sum_{j\in K_i} X_j + \sum_{j\in K_k\setminus K_i} X_j|]\\
    &\leq \|g''\|_\infty \sum_{i\in I}\sum_{k,j\in K_{i}}\mathbb{E}[|X_{i}X_{k}|]\mathbb{E}[|X_j|]\\
\end{align*}

Combing those and use \Cref{lem:stein_equation_normal_bound} gives the desired result.  

\end{proof}

\subsubsection{Triangles in $\mathcal{G}(n,p)$}
For the number of triangles in $G=\mathcal{G}(n,p)$, we consider the following set up.  

Let index set be 
\[
\Gamma_n = \{(u,v,w): 1\leq u<v<w\leq n\}
\]

and let 

\[
Y_\alpha = Y_{u,v,w} = \mathbf{1}(u\sim v) \mathbf{1}(v\sim w) \mathbf{1}(w\sim u)
\]

which equals 1 if and only if $uvw$ is a triangle. Also let $T$ be:
\[
T = \sum_{\alpha \in \Gamma_n} Y_\alpha
\]

Again if $|\{u,v,w\}\cap \{a,b,c\}|\leq 1$, then $Y_\alpha$ and $Y_\beta$ are independent. Let $K_\alpha$ contain $\alpha$ and all those tuples $\beta$ such that $|\alpha\cap \beta|\geq 2$. Recall from \Cref{thm:stein_equation_normal_dependent} that we need to have centered random variables with variance of sum being $1$. So, we standaridise to obtain $X_\alpha$:

\[
    X_{\alpha}=\frac{Y_{\alpha}-p^{3}}{\sqrt{\mathrm{Var}(T)}}
\]

A straightforward calculation shows:
\[
    \sigma^{2}=\operatorname{Var}(T)={\binom{n}{3}}p^{3}[1-p^{3}+3(n-3)p^{2}(1-p)]
\]

to see this, note $\mathrm{Var}(T) = \sum_\alpha \mathrm{Var}(Y_\alpha) + \sum_{\alpha\neq \beta} \mathrm{Cov}(Y_\alpha, Y_\beta)$, where $\mathrm{Var}(Y_\alpha) = p^3(1-p^3)$ and $\mathrm{Cov}(Y_\alpha, Y_\beta) = p^5(1-p)$ if $\alpha$ and $\beta$ share exactly two vertices, and 0 otherwise (see proof of \Cref{thm:triangle_poisson_dependent}). Now we can write as before $W = \sum_{\alpha \in \Gamma_n} X_\alpha$ with:

\[
    W_{\alpha}=\sum_{\beta\notin K_{\alpha}}X_{\beta}{\mathrm{~and~}}Z_{\alpha}=\sum_{\beta\in K_{\alpha}}X_{\beta}
\]

To obain a dissociated decomposition of $W$, we further deocompose $W_\alpha$ as follows:
\[
    W_{\alpha}=\sum_{\gamma\in K_{\alpha}\cup K_{\beta}}X_{\gamma}+\sum_{\gamma\in K_{\beta}\setminus K_{\alpha}}X_{\gamma}
\]

Applying \Cref{thm:stein_equation_normal_dependent} gives the following result:

\begin{theorem}\label{thm:triangle_gnp_normal}
    Let $W$ be the standardised number of triangles in $\mathcal{G}(n,p)$ with $n\geq 3$ and $p\in (0,1)$. Then for any continuous and piecewise continuously differentiable $h$ with $\mathbb{E}[|h(X)|]<\infty$: 
    \[
        |\mathbb{E} [h(W)]- \mathbb{E}[h(X)]| \leq \frac{4}{3}\times\frac{29n^{5}p^{3}}{\sigma^{3}}||h^{\prime}||
    \]

    If $p$ does not depend on $n$, then this expression is of order $\mathcal{O}(n^{-1})$.
\end{theorem}

\begin{proof}
    Using \Cref{thm:stein_equation_normal_dependent} which gives:
    \begin{equation*}
        \left|\mathbb{E}[h(W)] - \mathbb{E}[h(X)]\right| \leq 4||h^{\prime}||\sum_{\alpha\in I}\sum_{\beta,\gamma\in K_{\alpha}}\left(\mathbb{E}[|X_{\alpha}X_{\beta}X_{\gamma}|]+\mathbb{E}[|X_{\alpha}X_{\beta}|]\mathbb{E}[|X_{\gamma}|]
        \right)
    \end{equation*}
    First, computing explicitly the expectations leads to:
    \begin{equation*}
        \mathbb{E}[|X_{\alpha}|]={\frac{2}{\sigma}}p^{3}(1-p^{3}),\quad\mathbb{E}[|X_{\alpha}|^{2}]={\frac{1}{\sigma^{2}}}p^{3}(1-p^{3})
    \end{equation*}
    and
    \begin{equation*}
        \mathbb{E}[|X_{\alpha}^{3}|]={\frac{1}{\sigma^{3}}}(p^{3}(1-p^{3})^{3}+(1-p^{3})p^{9})\leq2{\frac{p^{3}}{\sigma^{3}}}
    \end{equation*}
    where we have used the crude bound $|X_\alpha| \leq \frac{1}{\sigma}$ (as the numerator is less than $1$). Now for the first double sum, we have:
    \begin{align*}
        \sum_{\alpha\in I}\sum_{\beta,\gamma\in K_{\alpha}}\mathbb{E}[|X_{\alpha}X_{\beta}X_{\gamma}|] &\leq \sum_{\alpha\in I} \mathbb{E}[|X_\alpha|^3] + 3\sum_{\alpha\in I}\sum_{\beta \in K_\alpha, \beta \neq \alpha} \mathbb{E}[|X_\alpha^2 X_\beta|] + \sum_{\alpha \in I} \sum_{\beta, \gamma \in K_\alpha\setminus \{\alpha\}, \beta \neq \gamma} \mathbb{E}[|X_\alpha X_\beta X_\gamma|]\\
        &\leq \binom{n}{3} \frac{2p^3}{\sigma^3} + 3\binom{n}{3} (3n) \frac{1}{\sigma} \frac{p^3(1-p^3)}{\sigma^2} + \binom{n}{3} (3n)^2 \left(\frac{1}{\sigma}\right)^2 \frac{2}{\sigma}p^{3}(1-p^{3}) \\
        &\leq \binom{n}{3} \frac{22n^2p^3}{\sigma^3} < \frac{11n^5p^3}{3\sigma^3}
    \end{align*}

    where we used the fact that $|K_\alpha| = 3n-3 + 1 \leq 3n$ and $|X_\alpha| \leq \frac{1}{\sigma}$ along with the expectations computed above.   

    For the second double sum, we have:
    \begin{align*}
        \sum_{\alpha\in I}\sum_{\beta,\gamma\in K_{\alpha}}\mathbb{E}[|X_{\alpha}X_{\beta}|]\mathbb{E}[|X_{\gamma}|] &\leq \frac{1}{\sigma}  \sum_{\alpha\in I}\sum_{\beta,\gamma\in K_{\alpha}}\mathbb{E}[|X_{\beta}|] \mathbb{E}[|X_{\gamma}|] \\
        &\leq \frac{1}{\sigma} \binom{n}{3} (3n)^2 \left(\frac{2}{\sigma}p^{3}(1-p^{3})\right)^2 \\
        &=\binom{n}{3} \left(\frac{36n^2}{\sigma^3}p^{3}(1-p^{3})\right) \\
        &< \frac{6n^5}{\sigma^3} p^3
    \end{align*}
    Summing those two gives the desired result.
\end{proof}


\subsubsection{Dense and Sparse Regimes}  

Note in \Cref{thm:triangle_gnp_normal}, we requie the factor $\frac{n^5}{\sigma^3}$ to be small in order to obtain a useful bound. This is the case when $p$ does not depend on $n$, as $\sigma^2 \sim n^4$. However, if for example $p=1/n$, then $\sigma$ is of order $\mathcal{O}(1)$, so the bound can be meaningless.  Those two cases in fact correspond to the dense and sparse regimes of $\mathcal{G}(n,p)$.  

\begin{definition}
    We say a function $f(n)$ satisfies:
    \[
        f(n) = \Theta(g(n))
        \]

    if there exists $c_1, c_2 > 0$ and $n_0$ such that $c_1 g(n) \leq f(n) \leq c_2 g(n)$ for all $n\geq n_0$.
\end{definition}

We can now define the dense and sparse regimes for Erd\"os-R\'enyi random graphs.

\begin{definition}
    Let $G=\mathcal{G}(n,p)$. We have the following:
    \begin{itemize}
        \item $G$ is \textbf{sparse} if $p=\Theta(c/n)$ for some $c=\Theta(1)$ or equivalently $\binom{n}{2}p=\Theta(1)$.
        \item $G$ is \textbf{dense} if $p=\Theta(1)$ or equivalently $\binom{n}{2}p=\Theta(n^2)$.
        \item $G$ is \textbf{moderately dense} if $p=\Theta(c\log n/n)$ for some $c=\Theta(1)$ or equivalently $\binom{n}{2}p=\Theta(n\log n)$.
    \end{itemize}
\end{definition}

Therefore, as discussed above, the Poisson approximation is more useful in the sparse regime, while the normal approximation is more useful in the dense regime.  


\newpage
\bibliographystyle{apalike}
\bibliography{bibliography3.bib}



\end{document}