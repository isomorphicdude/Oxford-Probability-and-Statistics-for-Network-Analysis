\documentclass{article}

\input{includes/commands.tex}
\input{includes/theoremstyle.tex}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[colon, sort&compress]{natbib}
\title{Week 5}

\date{\today}


\begin{document}

\maketitle

\section{Threshold Behaviours} 

We begin with an example of a threshold behaviour in a random graph.  

\begin{example}
    In $\mathcal{G}(n,p)$, if $p=\frac{c}{n}$ with $c<1$, then the largest connected component is of size $O(\log n)$, whereas if $c>1$, then the largest connected component is of linear size with probability $1$ as $n\to \infty$.      
\end{example}

To show this, we use the approach in \citep{krivelevich2012phase} and use the Depth-First Search (DFS) algorithm on the random graph. 

\subsection{Depth-First Search}
The DFS operates on the vertex set $V$ with three sets of vertices at each step:
\begin{itemize}
    \item $S$ is the set that has been explored
    \item $T$ is the set of vertices to be explored
    \item $U = V\setminus (S\cup T)$ is the set of vertices under investigation
\end{itemize}

The algorithm starts with $S=\emptyset$, $T=V$ and $U=\emptyset$. At each step, if $U$ is nonempty, let $v$ be the last vertex added to $U$, we perform the following:
\begin{enumerate}
    \item If $v$ has a neighbour $u$ in $T$, move $u$ from $T$ to $U$.
    \item If $v$ has no neighbour in $T$, move $v$ from $U$ to $S$.
\end{enumerate}

If $U$ is empty, we pick a vertex $v$ from $T$ and move it to $U$ (select the vertex with the smallest index). If $U\cup T=\emptyset$, query all remaining pairs in $S$ and stop.  

The period of time between two consecutive emptyings of $U$ is called an \textbf{epoch}, which reveals a connected component.  

\begin{proposition}
    The DFS algorithm has the following properties:
    \begin{enumerate}
        \item The number of epochs is equal to the number of connected components.
        \item There are no edges between sets $S$ and $T$ at any time.
        \item The set $U$ always spans a path.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Those directly follow from the algorithm.
\end{proof}

This particular formulation of the DFS algorithm allows for the application on an Erd\H{o}s-R\'{e}nyi random graph, where we query the existence of an edge between two vertices by a Bernoulli trial with probability $p$.  

Denote $X_i\in \{0,1\}$ as the outcome of the $i$-th query, with each $X_i \sim \text{Bernoulli}(p)$ for $i=1, \ldots, \binom{n}{2}$. 

\begin{proposition}
    Let $S(t), U(t), T(t)$ be the sets at time $t$ in the DFS algorithm. Then, 
    \begin{equation*}
        |U(t)|\leq1+\sum_{i=1}^{t}X_{i} \quad \mathrm{and} \quad |S(t)\cup U(t)|\geq\sum_{i=1}^{t}X_{i}
    \end{equation*}
    At all times, $|S(t)|+|U(t)|+|T(t)|=n$.
\end{proposition}

The $1$ comes from the initial vertex $v$ in $U$ at the beginning of each epoch. 

\subsection{Small Connected Components}

We will first show that the size of any connected component cannot exceed some constant associated with the probability $p$ with high probability as $n\to \infty$.  

\begin{theorem}\label{thm:small_connected_components}
    Let $\varepsilon \in (0,1)$ be fixed and let $G\sim \mathcal{G}(n,p)$ with $p=\frac{1-\varepsilon}{n}$, then any connected component $C$ of $G$, satisfies
    \begin{equation*}
        \mathbb{P}(|C|\leq \frac{7}{\varepsilon^2}\log n)\to 1 \quad \text{as} \quad n\to \infty
    \end{equation*}
\end{theorem}

The proof of this theorem is based on the following lemma, which bounds the number of successes $W$ in an interval of length $\ell$ during a sequence of Bernoulli trials.  

\begin{lemma}\label{lem:bernoulli_trials}
    Let $X_1, \ldots, X_N \overset{i.i.d.}{\sim} \text{Bernoulli}(p)$. Let $k, \ell \in \mathbb{N}$ with $k > \ell p$. Then, define the random variable $Y_t$ as:
    \begin{equation*}
        Y_t = \sum_{i=t}^{t+\ell-1}X_i
    \end{equation*}
    for $t=1, \ldots, N-\ell+1$. And the random variable $W$ as:
    \begin{equation*}
        W = \sum_{t=1}^{N-\ell+1} \mathbf{1}_{\{Y_t\geq k\}}
    \end{equation*}
    Then,
    \begin{equation*}
        \mathbb{P}(W\geq 1)\leq (N-\ell+1)\exp\left(-\frac{(k-\ell p)^2}{2(\ell p + (k - \ell p ) / 3)}\right)
    \end{equation*}
\end{lemma}

We delay the proof of this lemma to the end of this section and first prove \Cref{thm:small_connected_components}.  

\begin{proof}[Proof of \Cref{thm:small_connected_components}]
    Assume for the sake of contradiction that $G$ contains a connected component $C$ with more than $\frac{7}{\varepsilon^2}\log n$ vertices.  Let $k+1=\lfloor \frac{7}{\varepsilon^2}\log n \rfloor + 1$ be the smallest integer which is larger than $\frac{7}{\varepsilon^2}\log n$. We show the probability of such a component to exist tends to $0$ as $n\to \infty$.  

    \medskip

    During the epoch when $C$ was created, consider the moment when the algorithm has received a positive response for the $(k+1)$-th time (the $k+1$-th vertex of $C$ is found). 
    % Let $\Delta S = S\cap C$ be the set of vertices in $C$ that have been found before this moment, then $|\Delta S \cup U| = k$, 
    Before this moment the algorithm has recevied exactly $k$ positive responses.  

    The total number of queries up to this moment during this epoch is at most

    \begin{equation*}
        \binom{k}{2} + k(n-k) < kn
    \end{equation*}

    Set $\ell = kn$ and $N=\binom{n}{2}$, then by \Cref{lem:bernoulli_trials}, we have such a component $C$ with probability at most

    \begin{align*}
        \binom{n}{2} \exp \left( -\frac{(k-\ell p)^2}{2(\ell p + (k-\ell p)/3)} \right) &= \binom{n}{2} \exp\left(-\frac{k\varepsilon^2}{2(1-2\varepsilon/3)}\right) \\
        &\leq \binom{n}{2} \exp\left(\frac{k\varepsilon^2}{2(1-2\varepsilon/3)} - \frac{7 \log n}{2 (1-2\varepsilon/3)}\right) \\
    \end{align*}

    as $k\geq \frac{7}{\varepsilon^2}\log n - 1$. Now note $\binom{n}{2} < n^2$ and $\varepsilon<1$, so the probability of such a component $C$ existing is at most
    \begin{equation*}
        n^2 \exp\left(\frac{k\varepsilon^2}{2(1-2\varepsilon/3)}\right) n^{-7/2(1-2\varepsilon/3)} < e^{3/2} n^{-3/2}
    \end{equation*}

    Letting $X$ denote the number of connected components of size at least $\frac{7}{\varepsilon^2}\log n$, we have by Markov's inequality that
    \begin{equation*}
        \mathbb{P}(X\geq 1) \leq \mathbb{E}(X) < e^{3/2} n^{-1/2}
    \end{equation*}

    which tends to $0$ as $n\to \infty$.
\end{proof}

To show \Cref{lem:bernoulli_trials}, we require the following Chernoff bound.  

\begin{lemma}\label{lem:chernoff}
    If $X\sim \text{Binomial}(N,p)$, then for any $N, k >0$,
    \begin{equation*}
        \mathbb{P}(X-N p\geq k)\leq \exp\left(-{\frac{k^{2}}{2(N p+k/3)}}\right)
    \end{equation*}
\end{lemma}

The proof of this lemma is given in problem sheet 0.  

\begin{proof}[Proof of \Cref{lem:bernoulli_trials}]
    Let $W$ denote the number of intervals of length $\ell \in \{1, \ldots, N\}$ in which at least $k$ of the random variables $X_i$ take the value 1. We want to bound $\mathbb{P}(W \geq 1)$ from above.

    Using Markov's inequality,
    \[
    \mathbb{P}(W \geq 1) \leq \mathbb{E}(W).
    \]
    
    Now
    \[
    \mathbb{E}(W) = (N - \ell + 1)\mathbb{P}\left(\sum_{i=1}^{\ell} X_i \geq k \right).
    \]
    
    From \Cref{lem:chernoff}, we have
    \[
    \mathbb{P}\left(\sum_{i=1}^{\ell} X_i \geq k\right) = \mathbb{P}\left(\sum_{i=1}^{\ell} X_i - \ell p \geq k - \ell p\right) \leq \exp \left(-\frac{(k-\ell p)^2}{2(\ell p+(\ell p)^2/3)}\right)
    \]
    
    Combining these bounds yields the assertion.
\end{proof}


\begin{remark}
    The proof of \Cref{thm:small_connected_components} is based on the fact that the DFS algorithm must receive $k$ positive responses during \textbf{an interval/window within the epoch for that connected component} among the queries to ensure a size of at least $k$.
\end{remark}


\subsection{Large Connected Components}

We also state another result from \citep{krivelevich2012phase} that shows the existence of a linear size connected component with high probability as $n\to \infty$.  

\begin{theorem}
    Let $\varepsilon \in (0,\frac{1}{10})$ be fixed and let $G\sim \mathcal{G}(n,p)$ with $p=\frac{1+\varepsilon}{n}$, let the largest connected component be $C_{\max}$. Then,
    \begin{equation*}
        \mathbb{P}(|C_{\max}|\geq \frac{1}{5}\varepsilon^2 n)\to 1 \quad \text{as} \quad n\to \infty
    \end{equation*}
\end{theorem}

The proof can be found in the lecture notes which is not examinable.

\newpage 

\section{Shortest Paths}
In this section, we study the distribution of the length of the shortest path between two vertices in a Newman-Watts-Strogatz (NWS) random graph. The important idea is to use continuous-time analysis for discrete-time processes. 

This section will mainly consist of heuristic arguments based on \citep{Reinert2001}.

\subsection{First Simplification}

We consider the continuous circle model from \citep{Newman_2000}: let $C$ be a circle of circumference $L$. A $\mathrm{Poisson}(L\rho/2), L\rho>1$ number of shortcuts are added uniformly at random to the circle. The chords between points have length zero. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/newman1999.png}
    \caption{The continuous circle model \citep{Newman_2000}}
    \label{fig:newman1999}
\end{figure}

For two randomly chosen points $P$ and $Q$ on the circle, we are interested in the distribution of the length of the shortest path $\mathcal{D}$ between them.  

\subsection{Second Simplification}
The mental picture that is useful for this problem is to imagine two kinds of liquid entering the network at points $P$ and $Q$ simultaneously and flowing along the paths. The first time the two liquids meet is roughly half the length of the shortest path.  

To make this more formal, we consider a pure growth (Yule) process.

\begin{definition}
    A \textbf{pure growth (Yule) process} $M(t)$ is a continuous-time Markov chain with state space $\mathbb{N}$ with rate $\rho$ if it satisfies $S(0) = k \geq 1$ and
    \begin{equation*}        
        M(t) = \sum_{i=1}^k N_t^{(i)}
    \end{equation*}
    where $N_t^{(i)}$ are independent Poisson processes with rate $\rho$.
\end{definition}

\begin{remark}
    The pure growth process can also be viewed as the following branching process: at time $t$, each individual waits for an exponential time with rate $\rho$ and then splits into two individuals.
\end{remark}

In the current context, the pure growth process $M(t)$ should have a similar behaviour to the liquid (we only consider starting from $P$ and ignore $Q$ for now):

\begin{itemize}
    \item The process goes in both directions along the circle
    \item Offspring arise when taking shortcuts and we assert that shortcuts take zero time to traverse
    \item The offspring of the process should be thought as the intervals spanned on the circle (\Cref{fig:newman1999})
\end{itemize}

Therefore, we see that the process should have rate $2\rho$, as for each direction, the probability of encountering a shortcut is $2 \times (1/L) \times (\rho L/2) = \rho$, since a shortcut has two endpoints and they are uniformly distributed with number of shortcuts drawn from $\mathrm{Poisson}(L\rho/2)$.  

It can be shown that 
\begin{proposition}
    If the pure growth process $M(t)$ has rate $2\rho$, then $\mathbb{E}[M(t)] = e^{2\rho t}$.
\end{proposition}  
\begin{proof}
    A proof can be found on Pg. 82 of \citep{Norris_1997}.
\end{proof}

Now if $N(t)$ is another pure growth process with the same rate starting from $Q$, then we say the two processes meet each other if their offsprings (which are intervals on the circle) overlap/intersect. If we further let $V_t$ denotes the number of intersecting pairs of intervals at time $t$, then the event $\{V_t=0\}$ is (roughly) equivalent to the shortest path between $P$ and $Q$ being longer than $2t$:
\begin{equation}
    \mathbb{P}(\mathcal{D}>2t) \approx \mathbb{P}(V_t=0)
\end{equation} 

So we are interested in the distribution of $V_t$. To do this, we need to consider the total length covered by the offsprings at time $t$.  

\begin{proposition}\label{prop:total_length}
    Let $s(t)$ denote the total length covered by $M(t)$ at time $t$. Then, we can approximate it by 
    \begin{equation}
        \mathbb{E}[s(t)] \approx \frac{1}{\rho} e^{2\rho t}
    \end{equation}

    We have similar results for $N(t)$ and $u(t)$.  
\end{proposition}

The proof is based on the following theorem, which can be found in \hyperlink{http://galton.uchicago.edu/ Ìƒlalley/Courses/312/Branching.pdf}{this note}.

\begin{theorem}\label{thm:branching}
    Let $M(t)$ be a pure growth process with rate $2\rho$, then 
    \begin{equation*}
        \frac{M(t)}{e^{2\rho t}} \to W \quad a.s.
    \end{equation*}
    where $W$ follows an $\mathrm{Exp}(1)$ distribution.  
\end{theorem}

\begin{proof}[(Sketch) Proof of \Cref{prop:total_length}]
    Write
    \begin{equation*}
        \frac{M(t-h)}{M(t)}=\frac{M(t-h)e^{-2\rho(t-h)}}{M(t)e^{-2\rho t}}e^{-2\rho h}
    \end{equation*}
    and recall \Cref{thm:branching}. Then,
    \begin{equation*}
        \operatorname*{lim}_{t\rightarrow\infty}\frac{M(t-h)}{M(t)}=e^{-2\rho h}\mathrm{\,almost\,surely.}
    \end{equation*}
    If $X(t)$ is the average age at time $t$, then we have 
    \begin{equation*}
        \mathbb{P}(X(t)<h) = \frac{M(t)-M(t-h)}{M(t)}
    \end{equation*}
    where the numerator represents the number of offpsrings born in the interval $[t-h,t]$, hence has ages less than $h$. Therefore, we see $X(t)$ has an exponential distribution with rate $2\rho$ approximately.  So the average age is approximately $\frac{1}{2\rho}$.   

    Now we note an offspring with age $h$ can cover an interval of length $2h$ (as it extends in both directions) and there are on average $e^{2\rho h}-1$ offsprings at time $t$ (as the process starts with one individual). Therefore, the total length covered by $M(t)$ at time $t$ is approximately $\frac{1}{2\rho}e^{2\rho t}$.
\end{proof}  

\begin{figure}
        \centering
        \begin{tikzpicture}
                \coordinate (M) at (0,0);
                \coordinate (A) at (0,2);
                \coordinate (C) at (135:2);
                \coordinate (D) at (45:2);
                \coordinate (E) at (60:2);
                \coordinate (F) at (15:2);
                \begin{scope}
                \clip[draw] circle [radius=2];
                \draw [step=2,very thin,gray](-2,-2) grid (2,2);
                \end{scope}
                \draw (A) node[circle,inner sep=1.5pt,fill] {} (C) node[circle,inner sep=1.5pt,fill] {} (M) node[circle,inner sep=1.5pt,fill] {} (D) node[circle,inner sep=1.5pt,fill] {} (E) node[circle,inner sep=1.5pt, fill] {} (F) node[circle,inner sep=1.5pt, fill] {};
                \draw[-] (M) -- (C) (M) -- (A) (M) -- (D);
                \draw[dotted,|-|]
                    (0,2cm+10pt)
                    arc[start angle=90,end angle=135,radius=2cm+10pt]
                 node[midway,fill=white,sloped] {\tiny{$(0, 0+1/\rho)$}};
                \draw[dotted,|-|]
                    (60:2cm+10pt)
                    arc[start angle=60,end angle=15,radius=2cm+10pt]
                    node[midway,fill=white,sloped] {\tiny{$(a, a+1/\rho)$}};
        \end{tikzpicture}
        \caption{An interval can begin with $a$ and end with $a+1/\rho$ which intersects with the interval $(0, 0+1/\rho)$. It could also begin with $a$ and end with $a-1/\rho$.}
        \label{fig:total_length}
\end{figure}

We now approximate $V_t$. 

\begin{proposition}\label{prop:intersecting_pairs}
    Let $V_t$ denote the number of intersecting pairs of intervals at time $t$. Then, 
    \begin{equation}
        V_t \approx \frac{2}{L\rho} e^{4\rho t}
    \end{equation}
\end{proposition}

\begin{proof}[Sketch of Proof]
    We will approximate $V_t$ through its mean, namely:
    \begin{align*}
        \mathbb{E}[V_t] &= \mathbb{E}\left[\sum_{i=1}^{M(t)}\sum_{j=1}^{N(t)}\mathbf{1}_{\{I_i\cap J_j\neq \emptyset\}}\right] \\
        &\approx e^{4\rho t} \mathbb{P}(I_1\cap J_1\neq \emptyset)\\
        &= e^{4\rho t} \frac{2}{L\rho}
    \end{align*}

    since we have roughly $e^{4\rho t}$ pairs of intervals and each has length approximately $1/\rho$ by \Cref{prop:total_length}; and for any interval $(a, a+1/\rho)$, it intersects with another interval if $a$ falls into that interval, which happens with probability $2/(L\rho)$, since each interval has length $1/\rho$ and the circumference is $L$, with intersection happening in two directions (\Cref{fig:total_length}).
\end{proof}

We obtain an easier expression to work with by solving for $t$,  
\begin{equation}
    V_t \approx y
\end{equation}
Namely, set $y=2e^{2x}$, we can find $\tau_x$ such that $V_{\tau_x} \approx y$, where
\begin{equation*}
    \tau_{x}={\frac{1}{2\rho}}\left\{{\frac{1}{2}}\log(L\rho)+x\right\}
\end{equation*}

We are now ready to characterise the distribution of the shortest path through $V_t$.  

\subsection{Poisson Approximation}

Suppose at time $t$, there are intervals $I_1, \ldots, I_m$ with lengths $s_1, \ldots, s_m$ for process $M(t)$ and $J_1, \ldots, J_n$ with lengths $u_1, \ldots, u_n$ for process $N(t)$. Then, define
\begin{equation*}
    X_{ij} = \mathbf{1}(I_i\cap J_j\neq \emptyset) \mathbf{1}(I_i \nsubseteq J_j) \mathbf{1}(J_j \nsubseteq I_i)
\end{equation*}

We will approximate the sum 
\begin{equation*}
    V_{t} = \sum_{i=1}^{m}\sum_{j=1}^{n}X_{ij}
\end{equation*}
by a Poisson random variable with mean 
\begin{equation*}
    \lambda = \mathbb{E}[V_t] \approx \sum_{i=1}^{m}\sum_{j=1}^{n}\frac{2}{L\rho} \min(s_i, u_j)
\end{equation*}
which follows similarly from the proof of \Cref{prop:intersecting_pairs}. We have
\begin{theorem}
    With the same notation as above, the total variation distance is bounded by 
    \begin{equation*}
        d_{TV}(\mathcal{L}(V_t), \mathrm{Po}(\lambda)) \leq \frac{4}{L}(m+n) l_{su}
    \end{equation*}
    where $l_{su} = \max(\max_i s_i, \max_j u_j)$.  
\end{theorem}

\begin{proof}
    Recall the result of \Cref{thm:poisson_stein_local}. We need to have a set $A_\alpha$ such that $X_\alpha$ is independent of $\sum_{\beta\notin A_\alpha}X_\beta$ and $\eta_\alpha = \sum_{\beta \in A_\alpha}X_\beta$, then
    \begin{equation*}
        d_{TV}(\mathcal{L}(V_t), \text{Po}(\lambda)) \leq \sum_{\alpha\in I}[(p_{\alpha}\mathbb{E}(\eta_{\alpha})+\mathbb{E}(X_{\alpha}(\eta_{\alpha}-X_{\alpha}))]\operatorname*{min}\left(1,\lambda^{-1}\right)
    \end{equation*}
    Following this idea, we set 
    \begin{equation*}
        A_{(i,j)}=\{(k,\ell)\in I:\{k,\ell\}\cap\{i,j\}\neq\emptyset\}
    \end{equation*}
    with $\alpha = (i,j)$. Then, noting $|A_{(i,j)}| = m+n-2$, we have
    \begin{align*}
        \sum_{\alpha\in I}p_{\alpha}\mathrm{E}\eta_{\alpha} &\leq(m+n){\frac{2}{L}}l_{s u}\sum_{\alpha\in I}p_{\alpha} \\
        &\leq(m+n){\frac{2}{L}}l_{s u}\lambda
    \end{align*}
    On the other hand,
    \begin{align*}
        \sum_{\alpha\in I}\mathbb{E}(X_{\alpha}(\eta_{\alpha}-X_{\alpha})) &= \sum_{\alpha\in I} \sum_{\beta\neq \alpha \in A_\alpha} \mathbb{E}(X_{\alpha}X_{\beta}) \\
        &= \sum_{\alpha\in I} \sum_{\beta\neq \alpha \in A_\alpha} \mathbb{E}(X_{\alpha})\mathbb{E}(X_{\beta}) \\
        &\leq(m+n){\frac{2}{L}}l_{s u}\lambda
    \end{align*}
    where we used pair-wise independence. Now noting $\lambda>1$, we have the desired result.
\end{proof}

As a corollary, we obtain a mixed Poisson approximation for $V_{\tau_x}$ (this is mixed as we condition on the number of intervals and their lengths at time $\tau_x$). Given $M(\tau_x)=m$ and $N(\tau_x)=n$, we have

\begin{corollary}
    Using the same notations as above, we have
    \begin{equation*}
        d_{TV}(\mathcal{L}(V_{\tau_x}|M(\tau_x)=m, N(\tau_x)=n), \mathrm{Po}(\lambda)) \leq \frac{4}{L}(m+n) l_{su}
    \end{equation*}
    where $\lambda = \frac{2}{L}\sum_{i=1}^{m}\sum_{j=1}^{n} \min(s_i, u_j)$.
\end{corollary}

Therefore, the probability of having no intersecting pairs at time $\tau_x$ is approximately  

\begin{equation*}
    \mathbb{P} (V_{\tau_x} = 0 | M(\tau_x) = m, N(\tau_x) = n) \approx e^{-\lambda}
\end{equation*}

Here $\approx$ is used in the sense that the difference between the two expressions tends to $0$ as $L\to \infty$.  


\subsection{Unconditioning}

To estimate the distribution $V_{\tau_x}$, we need to remove the conditioning on $M(\tau_x)$ and $N(\tau_x)$.  

\begin{theorem}
    As $L\to \infty$, we have
    \begin{equation*}
        \mathbb{P}(\mathcal{D} > 2\tau_x) \approx \mathbb{P}(V_{\tau_x} = 0) \rightarrow \int_{0}^{\infty} \frac{e^{-y}}{1+e^{2x}y} dy
    \end{equation*}
    where $\mathcal{D}$ is the length of the shortest path between $P$ and $Q$ and $\tau_x=\frac{1}{2\rho}(\frac{1}{2}\log(L\rho)+x)$.
\end{theorem}

\begin{proof}
    Recall for $\lambda=\frac{2}{L}\sum_{i=1}^{m}\sum_{j=1}^{n} \min(s_i, u_j)$, we have
    \begin{equation*}
        \mathbb{P}(V_{\tau_x} = 0 | M(\tau_x) = m, N(\tau_x) = n) \approx e^{-\lambda}
    \end{equation*}
    If $M(\tau_x)=m$ and if $T_1^M, \ldots, T_m^M$ denote the times when the $m$ intervals of $M(\tau_x)$ were created, then for $0<v\leq \tau_x$,
    \begin{equation*}
        M(v) = \sum_{i=1}^m \mathbf{1}(v\geq T_i^M)
    \end{equation*}
    We have analogous results for $N(v)$. Now note that the length of an interval is $2$ times its age, so $s_i = 2 (\tau_x - T_i^M)$ and $u_j = 2 (\tau_x - T_j^N)$. Therefore, we have
    \begin{align*}
        \frac{2}{L} \sum_{i=1}^m \sum_{j=1}^n \min(s_i, u_j) &= \frac{2}{L} \sum_{i=1}^m \sum_{j=1}^n \min(2(\tau_x - T_i^M), 2(\tau_x - T_j^N)) \\
        &= \frac{4}{L} \sum_{i=1}^m \sum_{j=1}^n \min(\tau_x - T_i^M, \tau_x - T_j^N) \\
        &= \frac{4}{L} \sum_{i=1}^m \sum_{j=1}^n (\tau_x - \max(T_i^M, T_j^N)) \\
        &= \frac{4}{L} \sum_{i=1}^m \sum_{j=1}^n \int_0^{\tau_x} \mathbf{1}(v \geq T_i^M) \mathbf{1}(v \geq T_j^N) dv \\
        &= \frac{4}{L} \int_0^{\tau_x} M(v) N(v) dv
    \end{align*}  
    Therefore, integrating with $M$ and $N$ gives
    \begin{equation*}
        \mathbb{P}(V_{\tau_x} = 0) = \int_0^{\infty} \int_0^{\infty}\exp\left(-\frac{4}{L} \int_0^{\tau_x} M(v) N(v) dv\right) \mathrm{d}\mathbb{P}_M \mathrm{d}\mathbb{P}_N
    \end{equation*}

    To evaluate this integral, we recall \Cref{thm:branching}, $\frac{M(t)}{e^{2\rho t}} \to W$ where $W$ follows an $\mathrm{Exp}(1)$ distribution. Therefore, by approximating $e^{2\rho t} M(t) \to W$ almost surely and taking $W, W' \overset{i.i.d}{\sim} \mathrm{Exp}(1)$, we have  
    \begin{align*}
        \int_0^{\tau_x} M(v)N(v) dv &= \int_0^{\tau_x} e^{4\rho v} M(v)e^{-2\rho v} N(v)e^{-2\rho v} dv \\
        &\approx \int_0^{\tau_x} e^{4\rho v} W W' dv \\
        &= WW'\frac{1}{4\rho} (e^{4\rho \tau_x} - 1) \\
        &\approx \frac{1}{4\rho} WW' e^{2x} (\sqrt{L \rho})^2 \\
        &= \frac{L}{4} WW' e^{2x}
    \end{align*}
    Computing explicitly, we have:
\begin{equation*}
    \int_0^{\infty} \int_0^{\infty} \exp(-uy e^2x) e^{-u} e^{-y} \mathrm{d}u \mathrm{d}y = \int_0^{\infty} \frac{e^{-y}}{1+e^{2x}y} \mathrm{d}y
\end{equation*}
where $u$ and $y$ are the random variables corresponding to $W$ and $W'$ respectively.  
\end{proof}

Showing the approximation works is beyond the scope of this course, but can be found in \citep{Reinert2001}, which is the following:

\begin{theorem}
    If $T$ denotes the random variable with distribution
    \begin{equation*}
        \mathbb{P}(T>t) = \int_0^{\infty} \frac{e^{-y}}{1+e^{2t}y} \mathrm{d}y
    \end{equation*}
    and $D^* = \frac{1}{\rho} (\frac{1}{2}\log(L\rho) + T)$, then 
    \begin{equation*}
        \sup_t | \mathbb{P}(\mathcal{D} \leq t) - \mathbb{P}(D^* \leq t) | =\mathcal{O}((L\rho)^{-1/5} \log^2(L\rho))
    \end{equation*}
\end{theorem}

Note that $\mathbb{E}[T] = \frac{1}{2} \gamma \approx 0.2886$ where $\gamma$ is the Euler-Mascheroni constant.






\newpage
\bibliographystyle{apalike}
\bibliography{bibliography4.bib}
\end{document}