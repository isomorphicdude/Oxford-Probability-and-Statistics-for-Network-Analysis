\documentclass{article}

\input{/Users/isomorphicdude/My/Random_Latex/SC2-SM7-Network-Notes/includes/commands.tex}
\input{/Users/isomorphicdude/My/Random_Latex/SC2-SM7-Network-Notes/includes/theoremstyle.tex}
\usepackage{tikz}

\title{Week 3 \& 4 Stein's Method for Graphs}

\date{\today}

\begin{document}
% \author{\aut}
\maketitle

\section{Stein's Method for Graphs}

\subsection{Motivation}
Consider the problem of counting the number of triangles in the Erd\H{o}s-R\'{e}nyi random graph $G(n,p)$. We know that the expected number of triangles is $\binom{n}{3}p^3$. However, we would like to know the (approximate) distribution of the number of triangles. Central limit theorem is not applicable here, since the number of triangles is not a sum of independent random variables: 
\begin{equation*}
    T=\sum_{i<j<k} A_{ij}A_{jk}A_{ki}
\end{equation*}

the random variable $X_{i,j,k}=A_{ij}A_{jk}A_{ki}$ is not independent of one another as they may share edges.  

Stein's Method allows us to approximate the distribution of $T$ by another known distribution and we also get an idea of convergence rate.  

We introduce the general idea of Stein's Method and then consider the cases of a Poisson approximation and a normal approximation.

\subsection{Basic Ingredients}

\begin{unexaminable}
    This section is not examinable. The content of this section is based on \citep{anastasiou2022steins}.
\end{unexaminable}

The main idea works as follows: if we have a random variable $Z$ following an unknown distribution $Q$, we can show it satisfies some equation $\mathbb{E}[(\mathcal{T}f (Z))]=0, \forall f$ if and only if it follows distribution $P$. In particular, it is approximately $P$-distributed if we can show that $\mathbb{E}[(\mathcal{T}f (Z))]\approx 0$ for all $f$.  

More formally, we set up Stein's method for a target probability measure $P$ and any other measure $Q$. Let $\mathcal{G}(\mathcal{T})$ be a set of functions determined by a linear operator $\mathcal{T}$.  

\begin{definition}\label{def:stein_operator}
    The \textbf{Stein operator} $\mathcal{T}$ is a linear operator such that
    \begin{center}
        $P=Q$ if and only if $\mathbb{E}_{Z\sim Q}[(\mathcal{T}g(Z))]=0$ for all $g \in \mathcal{G}(\mathcal{T})$.
    \end{center}
    The set of functions $\mathcal{G}(\mathcal{T})$ such that $\mathbb{E}_{Z \sim P}[(\mathcal{T}g(Z))]=0, \forall g \in \mathcal{G}(\mathcal{T})$ is called the \textbf{Stein class} of $\mathcal{T}$.
\end{definition}

The equation $\mathbb{E}_{Z\sim Q}[(\mathcal{T}g(Z))]=0$ is called the \textbf{Stein identity}.  

With this set up, we can define the how close is $Z$ a $P$-distributed by introducing a measure for distance using the characterization above:

\begin{definition}
    The \textbf{Stein discrepancy} between $P$ and $Q$ is defined as\label{def:stein_discrepancy}
    \begin{equation*}
        \mathcal{D}(Q, \mathcal{T}, \mathcal{G})=\sup_{g \in \mathcal{G}(\mathcal{T})} \|\mathbb{E}_{Z\sim Q}[(\mathcal{T}g(Z))]\|^*
    \end{equation*}
    for some norm $\|\cdot\|^*$.
\end{definition}

The closer this discrepancy is to zero, the closer $Q$ is to $P$. 

Usually, this discrepancy is determined by various types of integral probability metrics (IPMs).

\begin{definition}\label{def:ipm}
    An \textbf{Integral Probability Metric (IPM)} is a function $d_{\mathcal{H}}$, s.t. 
    \begin{equation*}
        d_\mathcal{H}(P, Q) = \sup_{h \in \mathcal{H}} |\mathbb{E}_{X\sim P}[h(X)] - \mathbb{E}_{Z\sim Q}[h(Z)]|
    \end{equation*}
    The set $\mathcal{H} \subset L^1(P) \cap L^1(Q)$ is the class of test functions; if such function $d_\mathcal{H}$ is a metric, then $\mathcal{H}$ is called \textbf{measure determining}.
\end{definition}

An important example is the total variation distance, which we will use in the next section.  

\begin{example}
    The \textbf{total variation distance} is an IPM with $\mathcal{H}=\{1_A: A \in \mathcal{F}\}$, where $\mathcal{F}$ is the $\sigma$-algebra of events, which admits test functions $h(x) = I(x\in A)$. The total variation distance is defined as  
    \begin{equation}
        d_{TV}(P, Q) = \sup_{A \in \mathcal{F}} |P(A) - Q(A)|
        \label{eq:total_variation_distance}
    \end{equation}
    It also has an alternative representation with $\mathcal{H} = \{f: \|f\|_\infty \leq 1\}$:
    \begin{equation*}
        d_{TV}(P, Q) = \frac{1}{2} \sup_{f \in \mathcal{H}}\left|\int f dP - \int f dQ\right|
    \end{equation*}
    A similar representation when the distributions admit Radon-Nikodym derivatives $\frac{dP}{d\mu}=p$ and $\frac{dQ}{d\mu}=q$ with respect to a $\sigma$-finite measure $\mu$:
    \begin{equation*}
        d_{TV}(P, Q) = \frac{1}{2} \int |p - q| d\mu
    \end{equation*}
\end{example}

The proof for the second representation can be found \href{https://math.stackexchange.com/questions/3287889/show-that-the-total-variation-distance-of-probability-measures-mu-nu-is-equa}{here} and the third can be found \href{https://math.stackexchange.com/questions/1481101/definition-of-the-total-variation-distance-vp-q-frac12-int-p-qd-n}{here}.  

In fact, the concept of Stein discrepancy and general IPMs are related by Stein's equation.

\begin{definition}\label{def:stein_equation}
    The \textbf{Stein's equation} for $h\in \mathcal{H}$ is a functional equation:
    \begin{equation*}
        \mathcal{T}g (z) = h(z) - \mathbb{E}_{X\sim P}[h(X)]
    \end{equation*}
    evaluated over $z$ on the support of $P$, where $g$ is a solution to the Stein equation.
\end{definition}

If the solution $g$ exists, then we can take the expectation over $Q$ to get:
\begin{align*}
    \mathbb{E}_{Z\sim Q}[(\mathcal{T}g(Z))] &= \mathbb{E}_{Z\sim Q}[h(Z) - \mathbb{E}_{X\sim P}[h(X)]]\\
    &= \mathbb{E}_{Z\sim Q}[h(Z)] - \mathbb{E}_{X\sim P}[h(X)]\\
\end{align*}

Now taking the supremum over $h \in \mathcal{H}$, we recover the form of an IPM. 



\subsection{Stein-Chen Method for Poisson Approximation}  
Following the framework in the previous section, we can define the Stein operator (\Cref{def:stein_operator}) for Poisson approximation:
\begin{equation}
    \mathcal{T}g(z) = \lambda g(z) - zg(z)
\end{equation}

for $g: \mathbb{N}_0 \to \mathbb{R}$. This gives a characterization of the Poisson distribution with parameter $\lambda>0$:

\begin{proposition}
    \label{prop: direction1 poisson}
    If $Z\sim \text{Po}(\lambda)$, then $\mathbb{E}[(\mathcal{T}g(Z))]=0$ for all $g$ bounded.
\end{proposition}

\begin{proof}
This is a direct computation, noting the $Z=0$ gives zero expectation when $g$ is bounded:  
    \begin{align*}
        \lambda \mathbb{E}[g(Z+1)] &= \lambda \sum_{k=0}^\infty g(k+1) \frac{\lambda^k}{k!} e^{-\lambda}\\
        &= \lambda \sum_{k=1}^\infty g(k) \frac{\lambda^{k-1}}{(k-1)!} e^{-\lambda}\\
        &= \sum_{k=1}^\infty g(k) \frac{\lambda^k}{(k-1)!} e^{-\lambda}\\
        &= \sum_{k=1}^\infty g(k) \frac{\lambda^k}{k!} e^{-\lambda} \cdot k\\
        &= \mathbb{E}[Zg(Z)]\\
    \end{align*}
\end{proof}


The IPM we will use here is the total variation distance (\Cref{eq:total_variation_distance}), which leads to the Stein's equation (\Cref{def:stein_equation}):

\begin{equation}
    \mathcal{T}g(z) = I(j \in A) - \mathbb{E}[I(j \in A)]
    \label{eq:poisson_stein_equation}
\end{equation}

We show this equation has a \textit{unique} solution.

\begin{lemma}\label{lem:poisson_stein_equation}
    Given the Stein's equation above, we have a unique solution
    \begin{equation}
        g(z+1) = \frac{z!}{\lambda^{z+1}} e^\lambda \sum_{k=0}^z \frac{\lambda^k}{k!} (I(k\in A) - \mathbb{E}[I(k\in A)])
    \end{equation}

    this solution can also be written as 
    \begin{equation}
        g(z+1) = -\frac{z!}{\lambda^{z+1}} e^\lambda \sum_{k=z+1}^\infty \frac{\lambda^k}{k!} (I(k\in A) - \mathbb{E}[I(k\in A)])
    \end{equation}
\end{lemma}

\begin{proof}
    We first show the claimed solution satisfies the Stein's equation, which is easily verified by direct computation. To show uniqueness, let $z=0$ in \Cref{eq:poisson_stein_equation} and we get $\lambda g(1) = I(0\in A) - \mathbb{E}[I(0\in A)]$. So if $f$ is another solution to \Cref{eq:poisson_stein_equation}, then $f(1)=g(1)$. Now we can use induction to show $f(z)=g(z)$ for all $z\in \mathbb{N}_0$.
\end{proof}

To bound the total variation distance, we require additional bounds on $g$.  

\begin{lemma}\label{lem:poisson_stein_bound}
For the solution $g$ to the Stein's equation \Cref{eq:poisson_stein_equation}, we have:
\begin{equation}
    \sup_{k \in \mathbb{N}_0} |g(k)| \leq \min (1, \lambda^{-1/2})
    \label{eq:poisson_stein_bound1}
\end{equation}
and 
\begin{equation}
    \sup_{k \in \mathbb{N}_0} |g(k+1) - g(k)| \leq \min (1, \lambda^{-1})
    \label{eq:poisson_stein_bound2}
\end{equation}

Here $\lambda^{-1}$ is referred to as the \textit{magic factor}.
\end{lemma}

\begin{proof}
    The proof is omitted.
\end{proof}

Now we give the converse of \Cref{prop: direction1 poisson}.

\begin{proposition}\label{prop: direction2 poisson}
    If $\mathbb{E}[(\mathcal{T}g(Z))]=0$ for all $g$ bounded, then $Z\sim \text{Po}(\lambda)$.
\end{proposition}

\begin{proof}
    Since $g$ is an arbitrary bounded function, we take it as the solution obtained in \Cref{lem:poisson_stein_equation}, which is bounded by the previous lemma. Then taking the expectation of \Cref{eq:poisson_stein_equation} finishes the proof.
\end{proof}

Now we can bound the total variation distance between $Z$ and $\text{Po}(\lambda)$:
\begin{equation*}
    d_{TV}(\mathcal{L}(Z), \text{Po}(\lambda)) = \sup_{g} |\mathbb{E}[\lambda g(Z+1) - Zg(Z)]|
\end{equation*}

To see this is actually useful, we consider a bound for Bernoulli random variables. 

\begin{example}\label{ex:bernoulli_poisson}
    Let $X_1, \ldots, X_n$ be independent Bernoulli random variables, where $X_i \sim \text{Ber}(p_i)$ and $Z = \sum_{i=1}^n X_i$. Set $\lambda = \sum_{i=1}^n p_i$. Then we can compute the Stein's equation $\mathbb{E}[\lambda g(Z+1) - Zg(Z)]$.  Now since $X_i$ is binary and $Z-X_i$ is independent of $X_i$, we have:  
    \begin{align*}
        \mathbb{E}[Zg(Z)] &= \mathbb{E}[\sum_{i=1}^n X_i g(Z)]\\
        &= \sum_{i=1}^n \mathbb{E}[X_i g(Z + X_i - 1)]\\
        &= \sum_{i=1}^n \mathbb{E}[X_i] \mathbb{E}[g(Z + X_i - 1)]\\
        &= \sum_{i=1}^n p_i \mathbb{E}[g(Z + X_i - 1)]\\
    \end{align*}

    where in the second step we use the fact that $X_i=0$ when $g$ is bounded yields zero expectation. Thus, 
    \begin{align*}
        \mathbb{E}[\lambda g(Z+1) - Zg(Z)] &= \sum_{i=1}^n p_i \mathbb{E}[g(Z + 1)] - \sum_{i=1}^n p_i \mathbb{E}[g(Z + X_i - 1)]\\
        &= \sum_{i=1}^n p_i \mathbb{E}[\mathbb{E}[g(Z+1) - g(Z + X_i - 1) \mid X_i]]\\
        &= \sum_{i=1}^n p_i^2 \mathbb{E}[g(Z+1) - g(Z) \mid X_i=1]\\
        & \leq \sum_{i=1}^n p_i^2 \mathbb{E}[|g(Z+1) - g(Z)| \mid X_i=1]
    \end{align*}
    where we used conditional expectation.   
    Using using \Cref{lem:poisson_stein_bound} now gives 
    \[|\mathbb{E}[\lambda g(Z+1) - Zg(Z)]| \leq \min (1, \lambda^{-1}) \sum_{i=1}^n p_i^2\]
    So the total variation distance is bounded by
    \begin{equation*}
        d_{TV}(\mathcal{L}(Z), \text{Po}(\lambda)) \leq \min (1, \lambda^{-1}) \sum_{i=1}^n p_i^2
    \end{equation*}
    Note this bound is non-asympotic and holds for any $n$.
\end{example}

\subsubsection{Local dependence}  
When the random variables are not independent, the computation of $\mathbb{E}[Wg(W)]$ is more complicated.  

\begin{theorem}\label{thm:poisson_stein_local}
    Let $X_\alpha, \alpha \in I$ with each $X_\alpha \sim \text{Ber}(p_\alpha)$ and $Z = \sum_{\alpha \in I} X_\alpha$. Let $\lambda = \sum_{\alpha \in I}$. Suppose $\forall \alpha \in I$, there exists a set $A_\alpha \subseteq I$, s.t. $X_\alpha$ is independent of $\sum_{\beta \notin A_\alpha} X_\beta$. Define 
    \begin{equation*}
        \eta_\alpha = \sum_{\beta \in A_\alpha} X_\beta
    \end{equation*}
    Then the total variation distance is bounded by
    \begin{equation*}
        d_{TV}(\mathcal{L}(Z), \text{Po}(\lambda)) \leq \sum_{\alpha\in I}[(p_{\alpha}\mathbb{E}(\eta_{\alpha})+\mathbb{E}(X_{\alpha}(\eta_{\alpha}-X_{\alpha}))]\operatorname*{min}\left(1,\lambda^{-1}\right)
    \end{equation*}
\end{theorem}

\begin{proof}

    As before, we aim to bound the total variation distance by bounding $\mathbb{E}|\lambda g(Z+1) - Zg(Z)|$, where $g$ is the solution to the Stein's equation.  

    Let $Z_\alpha = Z - \eta_\alpha$, we compute the term $\mathbb{E}[Zg(Z)]$ as follows:  
    \begin{align*}
        \mathbb{E}[Zg(Z)] &= \mathbb{E}[\sum_{\alpha \in I} X_\alpha g(Z)]\\
        &= \sum_{\alpha \in I} \mathbb{E}[X_\alpha g(Z - X_\alpha + 1)]\\
        &= \sum_{\alpha \in I} \mathbb{E}[X_\alpha g(Z_\alpha + \eta_\alpha -X_\alpha +  1)]\\
        &= \sum_{\alpha \in I} \mathbb{E}[X_\alpha g(Z_\alpha + 1)] + \mathbb{E}[X_\alpha (g(Z_\alpha + \eta_\alpha -X_\alpha +  1) - g(Z_\alpha + 1))]\\
        &= \sum_{\alpha \in I} p_\alpha \mathbb{E}[g(Z_\alpha + 1)] \\
        &+ p_\alpha \mathbb{E}[g(Z+1)] - p_\alpha \mathbb{E}[g(Z+1)] \\
        &+ \mathbb{E}[X_\alpha (g(Z_\alpha + \eta_\alpha -X_\alpha +  1) - g(Z_\alpha + 1))] \\
    \end{align*}

    Rearranging the last term leads to 
    \[\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(Z + 1)] + \underbrace{p_\alpha \mathbb{E}[g(Z_\alpha+1) - g(Z+1)]}_{R_{1,\alpha}} + \underbrace{\mathbb{E}[X_\alpha (g(Z_\alpha + \eta_\alpha -X_\alpha +  1) - g(Z_\alpha + 1))]}_{R_{2, \alpha}}\] 

    Hence the equation we need to bound is
    \begin{equation*}
        \mathbb{E}[\lambda g(Z+1) - Zg(Z)] =  R_1 + R_2
    \end{equation*}

    For $R_1=\sum_\alpha R_{1,\alpha}$, we have
    \begin{align*}
        |R_1| &= \left|\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(Z_\alpha+1) - g(Z+1)]\right| \\
        &= \left|\sum_{\alpha \in I} p_\alpha \mathbb{E}[g(Z_\alpha + \eta_\alpha +1) - g(Z_\alpha+1)]\right|\\
        &\leq \sum_{\alpha \in I} p_\alpha |\mathbb{E}[g(Z_\alpha + \eta_\alpha +1) - g(Z_\alpha+1)]|\\
    \end{align*}
    Now we apply conditional expectation and telescoping sum to the inner term:
    \begin{align*}
        |\mathbb{E}[g(Z_\alpha + \eta_\alpha +1) - g(Z_\alpha+1)]| &= \left|\mathbb{E}\left[\mathbb{E}[\sum_{k=0}^{m-1} g(Z_\alpha + 1 + k+1) - g(Z_\alpha + 1 + k)\mid \eta_\alpha = m]\right]\right|\\
        &\leq \mathbb{E}\left[\mathbb{E}[\sum_{k=0}^{m-1} |g(Z_\alpha + 1 + k+1) - g(Z_\alpha + 1 + k)|\mid \eta_\alpha = m]\right]\\
        &\leq \mathbb{E}\left[\mathbb{E}[\sum_{k=0}^{m-1} \min (1, \lambda^{-1})\mid \eta_\alpha = m]\right]\\
        &= \min (1, \lambda^{-1}) \mathbb{E}[ \eta_\alpha]\\
    \end{align*}

    which gives the bound for $R_1$ as:
    \begin{equation*}
        |R_1| \leq \min (1, \lambda^{-1}) \sum_{\alpha \in I} p_\alpha \mathbb{E}[ \eta_\alpha]
    \end{equation*}

    For $R_2=\sum_\alpha R_{2,\alpha}$, we apply the method above to $g(Z_\alpha + \eta_\alpha - X_\alpha + 1) - g(Z_\alpha + 1)$, which gives the bound:
    \begin{equation*}
        |R_2| \leq \min (1, \lambda^{-1}) \sum_{\alpha \in I} p_\alpha \mathbb{E}[ X_\alpha (\eta_\alpha - X_\alpha)]
    \end{equation*}

    Combing those two bounds gives the desired result.
\end{proof}  

We are now ready to approximate the number of triangles in Erd\H{o}s-R\'{e}nyi random graph $G(n,p)$.  We use an index set $\Gamma_n$:

\begin{equation*}
    \Gamma_{n}=\{\alpha=(u,v,w):1\leq u<v<w\leq n\}
\end{equation*}

Then with the same notation as before, let 
$$
X_{\alpha}=X_{u,v,w} = A_{u v} A_{v w} A_{w u}
$$

which equals 1 if and only if $uvw$ is a triangle, each of the $X_\alpha$ is a Bernoulli random variable with parameter $p^3$. 

\begin{theorem}
    Let $Z = \sum_{\alpha \in \Gamma_n} X_\alpha$ and $\lambda = \binom{n}{3}p^3$.
    \begin{equation*}
        d_{TV}(\mathcal{L}(Z), \text{Po}(\lambda)) \leq \binom{n}{3}(3np^3 + 3np^2) \min (1, \lambda^{-1})
    \end{equation*}
\end{theorem}

\begin{proof}
    
\end{proof}


\subsection{Stein's Method for Normal Approximation}



\bibliographystyle{apalike}
\bibliography{bibliography3.bib}



\end{document}